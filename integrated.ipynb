{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import torchvision as tv\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "from pathlib import Path\n",
    "import h5py\n",
    "\n",
    "from sklearn.metrics import precision_score,f1_score,roc_curve,auc,accuracy_score\n",
    "from models.gaussian_blur import GaussianBlur\n",
    "from models.gamma_correction import GammaCorrection,HistogramEqualization\n",
    "from models.densenet import DenseNet\n",
    "from models import adamw, compare_auc_delong_xu, cosine_scheduler\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt \n",
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config(object):\n",
    "    def __init__(self):\n",
    "        self.name = 'Integrated'\n",
    "        self.dataset_name = 'mimic'\n",
    "        self.dataroot ='./DATASET/'\n",
    "        \n",
    "        self.save_path = './checkpoint/' + self.name\n",
    "        self.model_path = self.save_path + '/models'\n",
    "        self.decode_path = self.save_path + '/decoded_results'\n",
    "        self.val_path = self.save_path + '/val_results'\n",
    "        self.test_path = self.save_path + '/test_results'\n",
    "        \n",
    "        self.num_threads = 8\n",
    "        self.shuffle_dataset=True\n",
    "        self.random_seed=24\n",
    "\n",
    "\n",
    "        self.lr = 0.0005      \n",
    "        \n",
    "        self.serial_batches = False\n",
    "        self.phase='train'\n",
    "        \n",
    "        self.batch_size = 6\n",
    "        self.batch_size = 6\n",
    "        self.test_batch_size = 1\n",
    "        self.max_epochs = 500\n",
    "        self.save_every = 1     #epoch\n",
    "        self.plot_every = 1     # epoch to save decoded images\n",
    "\n",
    "        os.makedirs(self.save_path, exist_ok=True)\n",
    "        os.makedirs(self.model_path, exist_ok=True)\n",
    "        os.makedirs(self.decode_path, exist_ok=True)\n",
    "        os.makedirs(self.val_path, exist_ok=True)\n",
    "        os.makedirs(self.test_path, exist_ok=True)\n",
    "opt = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "phases = ['train', 'val','test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 5) (5000, 5) (1976, 5)\n",
      "Index(['Consolidation', 'Lung Opacity', 'Pleural Effusion', 'No Findings'], dtype='object')\n",
      "torch.Size([30000, 4])\n"
     ]
    }
   ],
   "source": [
    "root_dir = opt.dataroot\n",
    "raw_data_dir = f'{opt.dataroot}images/'\n",
    "int_data_dir = f'{root_dir}interim/'\n",
    "proc_data_dir = f'{root_dir}processed/'\n",
    "os.makedirs(proc_data_dir, exist_ok=True)\n",
    "os.makedirs(int_data_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# Path to csvfiles on training,validation and tetsing data\n",
    "csvpath = {phase: f'{root_dir}mimic_csv/{phase}.csv' for phase in phases}\n",
    "\n",
    "# Load data into dictionary of three Pandas DataFrames\n",
    "dframe = {phase: pd.read_csv(csvpath[phase]) for phase in phases}\n",
    "\n",
    "# Calculate sizes\n",
    "dataset_sizes = {phase: len(dframe[phase]) for phase in phases}\n",
    "print(dframe['train'].shape, dframe['val'].shape,dframe['test'].shape)\n",
    "\n",
    "input_size=224\n",
    "num_classes = 4\n",
    "num_samples = dframe['train'].shape[0]\n",
    "\n",
    "# Class names that will be predicted\n",
    "class_names = dframe['train'].iloc[:, 1:].columns\n",
    "print(class_names)\n",
    "\n",
    "# indices we will calculate AUC for, as in the CheXpert paper\n",
    "competition_tasks = torch.ByteTensor([1, 1, 1, 1]).bool()\n",
    "\n",
    "df = dframe['train'].iloc[:, 1:].copy()\n",
    "pos_weight = torch.Tensor([df[cl].sum() / df.shape[0] for cl in class_names])\n",
    "\n",
    "\n",
    "labels_array = df.iloc[:, 1:].copy()\n",
    "labels_array = {phase: dframe[phase].iloc[:, 1:].copy() for phase in phases}\n",
    "for phase in labels_array.keys():\n",
    "    labels_array[phase] = torch.FloatTensor(labels_array[phase].to_numpy())\n",
    "\n",
    "# labels_array = torch.FloatTensor(labels_array.to_numpy())  # needed when using cross-entropy loss\n",
    "print(labels_array['train'].shape)\n",
    "\n",
    "tforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        #GammaCorrection(0.8),\n",
    "        #HistogramEqualization(),\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'test': transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "#         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])}\n",
    "hdf50 = Path(f'{int_data_dir}/{phases[0]}_processed.h5')\n",
    "hdf51 = Path(f'{int_data_dir}/{phases[1]}_processed.h5')\n",
    "hdf52 = Path(f'{int_data_dir}/{phases[2]}_processed.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_images(img_paths, labels_array,input_size, phases=None, tforms=None):\n",
    "    \"\"\"\n",
    "    Saves compressed, resized images as HDF5 datsets\n",
    "    Returns\n",
    "        data.h5, where each dataset is an image or class label\n",
    "        e.g. X23,y23 = image and corresponding class labels\n",
    "    \"\"\"\n",
    "    if phases is None:\n",
    "        phases = ['train','val','test']\n",
    "    for phase in phases:\n",
    "        print(f'Processing {phase} files...')\n",
    "        with h5py.File(f'{proc_data_dir}/{phase}_processed.h5', 'w') as hf:\n",
    "            for i, img_path in enumerate(img_paths[phase]):\n",
    "                if i % 1000 == 0:\n",
    "                    print(f\"{i} images processed\")\n",
    "\n",
    "                img = Image.open(img_path+'.jpg').convert('RGB')\n",
    "                if tforms:\n",
    "                    img = tforms[phase](img)\n",
    "                hf.create_dataset(\n",
    "                    name=f\"X{i}\",\n",
    "                    data=img,\n",
    "                    shape=(3, input_size, input_size),\n",
    "                    maxshape=(3, input_size, input_size),\n",
    "                    compression=\"lzf\",\n",
    "                    shuffle=\"True\")\n",
    "                # Labels\n",
    "                hf.create_dataset(\n",
    "                    name=f\"y{i}\",\n",
    "                    data=labels_array[phase][i, :],\n",
    "                    shape=(num_classes,),\n",
    "                    maxshape=(num_classes,),\n",
    "                    compression=\"lzf\",\n",
    "                    shuffle=\"True\",\n",
    "                    dtype=\"i1\")\n",
    "    print('Finished!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf50 = Path(f'{proc_data_dir}/{phases[0]}_processed.h5')\n",
    "hdf51 = Path(f'{proc_data_dir}/{phases[1]}_processed.h5')\n",
    "hdf52 = Path(f'{proc_data_dir}/{phases[2]}_processed.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not (hdf50.is_file() or hdf51.is_file() or hdf52.is_file() ):\n",
    "    img_paths = {phase: raw_data_dir + dframe[phase].iloc[:, 0] for phase in phases}\n",
    "    proc_images(img_paths, labels_array,input_size,phases=phases, tforms=tforms)\n",
    "    \n",
    "hdf5paths = {phase: f'{proc_data_dir}{phase}_processed.h5' for phase in phases}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheXpertDataset(data.Dataset):\n",
    "    \"\"\"CheXpert dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, phase, num_samples, datapath):\n",
    "     \n",
    "        self.phase = phase\n",
    "        self.num_samples = num_samples\n",
    "        self.datapath = datapath\n",
    "        self.data = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if not self.data:    # open in thread\n",
    "            self.data = h5py.File(f'{self.datapath}', 'r')\n",
    "#         img = np.float32(self.data[f\"X{idx}\"][()])\n",
    "        img = self.data[f\"X{idx}\"][()] \n",
    "\n",
    "        labels = np.float32(self.data[f\"y{idx}\"][()])\n",
    "\n",
    "        return img, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    phase: CheXpertDataset(phase=phase, num_samples=dataset_sizes[phase],datapath=hdf5paths[phase])\n",
    "    for phase in phases}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_workers = 1  # Use with hdf5 files\n",
    "params = {'train': {'batch_size': opt.batch_size,\n",
    "                    'shuffle': True,\n",
    "                    'num_workers': num_workers,\n",
    "                    'pin_memory': True,\n",
    "                    'sampler': None,\n",
    "                    'drop_last':True},\n",
    "          'val': {'batch_size': opt.batch_size,\n",
    "                  'shuffle': True,\n",
    "                  'num_workers': num_workers,\n",
    "                  'pin_memory': True,\n",
    "                   'drop_last':True},\n",
    "           'test': {'batch_size': opt.test_batch_size,\n",
    "                  'shuffle': False,\n",
    "                  'num_workers': num_workers,\n",
    "                  'pin_memory': True,\n",
    "                   'drop_last':True}\n",
    "         }\n",
    "\n",
    "if params['train']['sampler'] is not None:\n",
    "    params['train']['shuffle'] = False\n",
    "    \n",
    "dataloaders = {phase: data.DataLoader(datasets[phase], **params[phase]) for phase in phases}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def down_pooling():\n",
    "    return nn.MaxPool2d(2)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_z=512      # number of dimensions in latent space.\n",
    "        \n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3,stride=1,padding=1),\n",
    "                                 nn.BatchNorm2d(64),\n",
    "                                 nn.ReLU(inplace=True)\n",
    "                                )\n",
    "        self.conv11 = nn.Sequential(nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3,stride=1,padding=1),\n",
    "                                 nn.BatchNorm2d(64),\n",
    "                                 nn.ReLU(inplace=True)\n",
    "                                )\n",
    "        \n",
    "        self.conv2=nn.Sequential(nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,stride=1,padding=1),\n",
    "                                 nn.BatchNorm2d(128),\n",
    "                                 nn.ReLU(inplace=True)\n",
    "                                )\n",
    "        self.conv22=nn.Sequential(nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3,stride=1,padding=1),\n",
    "                                 nn.BatchNorm2d(128),\n",
    "                                 nn.ReLU(inplace=True)\n",
    "                                )\n",
    "        self.conv3=nn.Sequential(nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3,stride=1,padding=1),\n",
    "                                 nn.BatchNorm2d(256),\n",
    "                                 nn.LeakyReLU(inplace=True)\n",
    "                                )\n",
    "        self.conv33=nn.Sequential(nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3,stride=1,padding=1),\n",
    "                                 nn.BatchNorm2d(256),\n",
    "                                 nn.ReLU(inplace=True)\n",
    "                                )\n",
    "        self.conv4=nn.Sequential(nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3,stride=1,padding=1),\n",
    "                                 nn.BatchNorm2d(512),\n",
    "                                 nn.ReLU(inplace=True)\n",
    "                                )\n",
    "        self.conv44=nn.Sequential(nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3,stride=1,padding=1),\n",
    "                                 nn.BatchNorm2d(512),\n",
    "                                 nn.ReLU(inplace=True)\n",
    "                                )        \n",
    "        self.conv5=nn.Sequential(nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3,stride=1,padding=1),\n",
    "                                 nn.BatchNorm2d(1024),\n",
    "                                 nn.ReLU(inplace=True)\n",
    "                                )\n",
    "        self.conv55=nn.Sequential(nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3,stride=1,padding=1),\n",
    "                                 nn.BatchNorm2d(1024),\n",
    "                                 nn.ReLU(inplace=True)\n",
    "                                )\n",
    "        self.down_pooling = nn.MaxPool2d(2)\n",
    "        \n",
    "        self.mu = nn.Linear(1024*14*14, 256)\n",
    "        self.logvar = nn.Linear(1024*14*14, 256)\n",
    "\n",
    "        \n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        :param mu: mean from the encoder's latent space\n",
    "        :param log_var: log variance from the encoder's latent space\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5*log_var) # standard deviation\n",
    "        eps = torch.randn_like(std) # `randn_like` as we need the same size\n",
    "        sample = mu + (eps * std) # sampling as if coming from the input space\n",
    "        return sample\n",
    "\n",
    "        \n",
    "    def forward(self,x):\n",
    "        \n",
    "        x=self.conv1(x)\n",
    "        x=self.conv11(x)\n",
    "        x = self.down_pooling(x)\n",
    "        \n",
    "        x=self.conv2(x)\n",
    "        x=self.conv22(x)\n",
    "        x = self.down_pooling(x)\n",
    "        \n",
    "        \n",
    "        x=self.conv3(x)\n",
    "        x=self.conv33(x)\n",
    "        x = self.down_pooling(x)\n",
    "\n",
    "        \n",
    "        x=self.conv4(x)\n",
    "        x=self.conv44(x)\n",
    "        x = self.down_pooling(x)\n",
    "        \n",
    "        x=self.conv5(x)\n",
    "        x=self.conv55(x)\n",
    "        \n",
    "        x = x.view(x.size(0),-1)\n",
    "        mu, logvar = self.mu(x), self.logvar(x)\n",
    "        x = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        return x,mu,logvar\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def up_pooling(in_channels, out_channels, kernel_size=2, stride=2):\n",
    "    return nn.Sequential(\n",
    "        nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(inplace=True)\n",
    "    )\n",
    "def conv_bn_leru(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "    return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.Linear(256,1024*14*14),\n",
    "                                nn.ReLU()\n",
    "                                )\n",
    "        self.up_pool6 = up_pooling(1024, 1024)\n",
    "        self.conv6 = conv_bn_leru(1024, 512)\n",
    "        self.up_pool7 = up_pooling(512, 512)\n",
    "        self.conv7 = conv_bn_leru(512, 256)\n",
    "        self.up_pool8 = up_pooling(256, 256)\n",
    "        self.conv8 = conv_bn_leru(256, 128)\n",
    "        self.up_pool9 = up_pooling(128, 128)\n",
    "        self.conv9 = conv_bn_leru(128, 64)\n",
    "\n",
    "        self.conv10 = nn.Conv2d(64, 1,1)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x  = self.fc(x)\n",
    "        x5 = x.view(x.size(0),1024,14,14)\n",
    "        \n",
    "        p6 = self.up_pool6(x5)\n",
    "        x6 = self.conv6(p6)\n",
    "\n",
    "        p7 = self.up_pool7(x6)\n",
    "        x7 = self.conv7(p7)\n",
    "\n",
    "        p8 = self.up_pool8(x7)\n",
    "        x8 = self.conv8(p8)\n",
    "\n",
    "        p9 = self.up_pool9(x8)\n",
    "        x9 = self.conv9(p9)\n",
    "        \n",
    "        output = self.conv10(x9)\n",
    "        output = self.relu(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def free_params(module: nn.Module):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "def frozen_params(module: nn.Module):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=Encoder()\n",
    "decoder=Decoder()\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "# decoder = decoder.to(device)\n",
    "frozen_params(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        init.xavier_normal_(m.weight)\n",
    "        init.constant_(m.bias, 0)\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.xavier_normal_(m.weight)\n",
    "        init.constant_(m.bias, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define the descriminator\n",
    "class Regressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Regressor, self).__init__()\n",
    "        self.n_z = 256\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(self.n_z, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(16, 4),\n",
    "#             nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Regressor(\n",
       "  (main): Sequential(\n",
       "    (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=128, out_features=128, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Linear(in_features=64, out_features=64, bias=True)\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Linear(in_features=64, out_features=32, bias=True)\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Linear(in_features=32, out_features=32, bias=True)\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Linear(in_features=32, out_features=16, bias=True)\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Linear(in_features=16, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=Regressor().to(device)\n",
    "model.apply(weight_init)\n",
    "# criterion = nn.BCELoss()\n",
    "# criterion = nn.BCEWithLogitsLoss(reduction='sum',pos_weight=pos_weight).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(**kwargs):\n",
    "    torch.cuda.empty_cache()\n",
    "    opt = Config()\n",
    "    print('loading the model...')\n",
    "\n",
    "    \n",
    "    epoch_size = int(num_samples /opt.batch_size)  # number of training examples/batch size\n",
    "#     optimizer = adamw.AdamW(model.parameters(), lr=opt.lr,weight_decay=1e-5)#weight_decay=1e-5  \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = opt.lr,weight_decay=5e-3)#,amsgrad=True\n",
    "    scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=5, eta_min=0.005 )\n",
    "    \n",
    "    #loading pretrained encoder\n",
    "    en_state = torch.load(os.path.join('./checkpoint/vae_wbc/models', 'train_model.pth'),map_location='cuda:5')\n",
    "    encoder.load_state_dict(en_state['enc_state_dict'])   \n",
    "\n",
    "    \n",
    "    try:\n",
    "        state = torch.load(os.path.join(opt.model_path, 'model.pth'),map_location='cuda:5')\n",
    "        model.load_state_dict(state['state_dict'])\n",
    "        print(\"Loaded pre-trained model with success.\")\n",
    "        e_counter=state['epoch']\n",
    "        best_acc = state['valid_acc_max']\n",
    "        print('Previously Trained for {} epoches'.format(e_counter))\n",
    "        print('Best val Accuracy till yet :',best_acc,'%')\n",
    "        e_counter+=1\n",
    "    except FileNotFoundError:\n",
    "        print(\"Pre-trained weights not found. Training from scratch.\")\n",
    "        e_counter=0\n",
    "        best_valid_loss = float('inf')\n",
    "        prev_loss=float('inf')\n",
    "        best_acc = 0.0\n",
    "\n",
    "  \n",
    "    for epoch in range(e_counter,opt.max_epochs):\n",
    "        model.train()\n",
    "        epoch_start_time = time.time()\n",
    "        print()\n",
    "        print('==================================================================')\n",
    "        print('-------------Epoch: {}/{}------------'.format(epoch,opt.max_epochs))\n",
    "        for phase in ['train','val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss=0.0\n",
    "            running_corrects = 0.0 \n",
    "            running_auc=0.0\n",
    "            \n",
    "            p_c=0\n",
    "            p_t=0\n",
    "            p_acc=0\n",
    "            \n",
    "            r_acc=0\n",
    "            r=0\n",
    "            for idx,batch in enumerate(dataloaders[phase]):\n",
    "                images,labels=batch\n",
    "                images,labels=images.to(device),labels.to(device)\n",
    "                \n",
    "                l=labels.to(\"cpu\").to(torch.int).numpy()\n",
    "                pos_w = []\n",
    "                for i in range(num_classes):\n",
    "                    z=o=0\n",
    "                    for j in range(opt.batch_size):\n",
    "                        if l[j][i] == 0:\n",
    "                            z += 1\n",
    "                        elif l[j][i] == 1:\n",
    "                            o += 1\n",
    "                    if o == 0:\n",
    "                        p_w = 1.0\n",
    "                    else:\n",
    "                        p_w = z/o\n",
    "                    pos_w.append(p_w)\n",
    "                \n",
    "                pos_w= torch.FloatTensor(pos_w)\n",
    "                criterion = nn.BCEWithLogitsLoss(reduction='sum',pos_weight=pos_w).to(device)\n",
    "                \n",
    "                if phase=='train':\n",
    "                    optimizer.zero_grad()\n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    enc_out,mu,logvar = encoder(images)\n",
    "                    outputs = model(mu)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    \n",
    "                    if phase=='train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                \n",
    "\n",
    "                preds=outputs.data > 0.5\n",
    "#                 preds = torch.sigmoid(outputs).data > 0.5\n",
    "#                 preds = outputs.data > 0.4\n",
    "                preds = preds.to(torch.float32)\n",
    "    \n",
    "        \n",
    "                auclist = []\n",
    "                p=preds.to(\"cpu\").to(torch.int).numpy()\n",
    "                \n",
    "                for i in range(opt.batch_size):\n",
    "                    label=labels[i].to('cpu').to(torch.int).numpy()\n",
    "                    pred= preds[i].to('cpu').to(torch.int).numpy()\n",
    "                    p_t += len(label)*opt.batch_size\n",
    "                    fpr,tpr,thresholds=roc_curve(label,pred,pos_label=1)\n",
    "                    AUC = auc(fpr, tpr)\n",
    "                    auclist.append(AUC)\n",
    "                    for j in range(len(label)):\n",
    "                        if label[j]==pred[j]:\n",
    "                            p_c += 1\n",
    "                    \n",
    "                    p_acc += p_c/p_t\n",
    "                    \n",
    "                \n",
    "                temp= np.array(auclist).mean()\n",
    "                running_auc += temp\n",
    "                \n",
    "                running_loss += loss.item() \n",
    "                \n",
    "                r_acc += accuracy_score(p, l)\n",
    "            \n",
    "#                 running_corrects += f1_score(labels.to(\"cpu\").to(torch.int).numpy() ,\n",
    "#                                              preds.to(\"cpu\").to(torch.int).numpy() , \n",
    "#                                              average=\"samples\")* images.size(0)\n",
    "               \n",
    "                \n",
    "                if (idx+1)%500 == 0:  \n",
    "                   # print(running_loss,r_acc,p_acc,running_auc)\n",
    "                    print(f'{phase}_batch {idx+1}/{len(dataloaders[phase])} Loss: {(running_loss/((idx+1))):.5f}'\n",
    "                              f' ACC: {(r_acc /(idx + 1))*100:.3f}%'\n",
    "                              f' p_acc: {(p_acc /(idx + 1))*100:.3f}%'\n",
    "                              f' AUC: {(running_auc /((idx + 1))):.3f}'\n",
    "                         )\n",
    "                    \n",
    "            #print(running_loss,r_acc,p_acc,running_auc)   \n",
    "            epoch_loss = running_loss / len(dataloaders[phase])\n",
    "            epoch_acc = (r_acc / len(dataloaders[phase]))*100\n",
    "            epoch_p_acc = (p_acc / len(dataloaders[phase]))*100                           \n",
    "            epoch_auc = running_auc/ len(dataloaders[phase])\n",
    " \n",
    "            print(f'{phase} Epoch Loss: {epoch_loss:.5f} ACC: {epoch_acc:.3f}% p_ACC: {epoch_p_acc:.3f}% AUC: {epoch_auc: .3f}')\n",
    "            with open('./checkpoint/Integrated/train_logs.txt', 'a') as file:\n",
    "                file.write('epoch: '+str(epoch)+',loss: '+str(epoch_loss)+',acc: '+str(epoch_acc)+',auc: '+str(epoch_auc) +'\\n')\n",
    "            print('.............')\n",
    "            state = {\n",
    "                    'epoch': epoch,\n",
    "                    'valid_acc_max': epoch_acc,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict()\n",
    "                    }\n",
    "\n",
    "            if phase == 'val':\n",
    "                if epoch_acc > best_acc:\n",
    "                    \n",
    "                    print('Val auc increased ({:.3f} --> {:.3f}). Saving model ...'.format(best_acc,epoch_acc))\n",
    "                    torch.save(state, os.path.join(opt.model_path, 'model.pth'))\n",
    "                    with open('./checkpoint/Integrated/val_logs.txt', 'a') as file:\n",
    "                        file.write('epoch: ' + str(epoch) + ',loss: ' + str(epoch_loss) + ',acc: ' + str(epoch_acc)+',auc: '+str(epoch_auc) + '\\n')\n",
    "                        \n",
    "                    best_acc = epoch_acc\n",
    "                epoch_time = int(time.time() - epoch_start_time)\n",
    "                print(f'-----------------Epoch cost time {epoch_time}s--------------------')\n",
    "#                 scheduler.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading the model...\n",
      "Loaded pre-trained model with success.\n",
      "Previously Trained for 18 epoches\n",
      "Best val Accuracy till yet : 48.07923169267704 %\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 19/500------------\n",
      "train_batch 500/5000 Loss: 5.12556 ACC: 78.667% p_acc: 93.170% AUC: 0.928\n",
      "train_batch 1000/5000 Loss: 5.29377 ACC: 77.967% p_acc: 93.032% AUC: 0.924\n",
      "train_batch 1500/5000 Loss: 5.37821 ACC: 77.333% p_acc: 92.911% AUC: 0.921\n",
      "train_batch 2000/5000 Loss: 5.54777 ACC: 76.642% p_acc: 92.795% AUC: 0.919\n",
      "train_batch 2500/5000 Loss: 5.58712 ACC: 76.640% p_acc: 92.697% AUC: 0.919\n",
      "train_batch 3000/5000 Loss: 5.62531 ACC: 76.617% p_acc: 92.635% AUC: 0.918\n",
      "train_batch 3500/5000 Loss: 5.67663 ACC: 76.610% p_acc: 92.586% AUC: 0.918\n",
      "train_batch 4000/5000 Loss: 5.78732 ACC: 76.383% p_acc: 92.541% AUC: 0.916\n",
      "train_batch 4500/5000 Loss: 5.81404 ACC: 76.156% p_acc: 92.490% AUC: 0.915\n",
      "train_batch 5000/5000 Loss: 5.92366 ACC: 75.727% p_acc: 92.438% AUC: 0.913\n",
      "train Epoch Loss: 5.92366 ACC: 75.727% p_ACC: 92.438% AUC:  0.913\n",
      ".............\n",
      "val_batch 500/833 Loss: 27.77150 ACC: 44.500% p_acc: 74.634% AUC: 0.705\n",
      "val Epoch Loss: 28.04587 ACC: 44.258% p_ACC: 74.682% AUC:  0.704\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 20/500------------\n",
      "train_batch 500/5000 Loss: 5.20820 ACC: 76.733% p_acc: 92.578% AUC: 0.924\n",
      "train_batch 1000/5000 Loss: 5.24309 ACC: 77.533% p_acc: 92.615% AUC: 0.925\n",
      "train_batch 1500/5000 Loss: 5.22293 ACC: 78.089% p_acc: 92.677% AUC: 0.925\n",
      "train_batch 2000/5000 Loss: 5.27802 ACC: 77.758% p_acc: 92.706% AUC: 0.923\n",
      "train_batch 2500/5000 Loss: 5.37602 ACC: 77.447% p_acc: 92.708% AUC: 0.922\n",
      "train_batch 3000/5000 Loss: 5.42339 ACC: 77.378% p_acc: 92.695% AUC: 0.922\n",
      "train_batch 3500/5000 Loss: 5.52124 ACC: 77.095% p_acc: 92.673% AUC: 0.920\n",
      "train_batch 4000/5000 Loss: 5.58721 ACC: 76.729% p_acc: 92.636% AUC: 0.919\n",
      "train_batch 4500/5000 Loss: 5.63279 ACC: 76.641% p_acc: 92.596% AUC: 0.918\n",
      "train_batch 5000/5000 Loss: 5.68319 ACC: 76.517% p_acc: 92.556% AUC: 0.917\n",
      "train Epoch Loss: 5.68319 ACC: 76.517% p_ACC: 92.556% AUC:  0.917\n",
      ".............\n",
      "val_batch 500/833 Loss: 31.41467 ACC: 46.267% p_acc: 75.251% AUC: 0.710\n",
      "val Epoch Loss: 31.31017 ACC: 46.178% p_ACC: 75.311% AUC:  0.714\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 21/500------------\n",
      "train_batch 500/5000 Loss: 4.68299 ACC: 80.400% p_acc: 94.185% AUC: 0.936\n",
      "train_batch 1000/5000 Loss: 4.73765 ACC: 80.217% p_acc: 94.000% AUC: 0.935\n",
      "train_batch 1500/5000 Loss: 4.90369 ACC: 79.433% p_acc: 93.840% AUC: 0.931\n",
      "train_batch 2000/5000 Loss: 5.06788 ACC: 78.875% p_acc: 93.674% AUC: 0.928\n",
      "train_batch 2500/5000 Loss: 5.11520 ACC: 78.773% p_acc: 93.548% AUC: 0.927\n",
      "train_batch 3000/5000 Loss: 5.21048 ACC: 78.300% p_acc: 93.445% AUC: 0.926\n",
      "train_batch 3500/5000 Loss: 5.25220 ACC: 78.138% p_acc: 93.366% AUC: 0.925\n",
      "train_batch 4000/5000 Loss: 5.32431 ACC: 77.913% p_acc: 93.288% AUC: 0.924\n",
      "train_batch 4500/5000 Loss: 5.41073 ACC: 77.700% p_acc: 93.218% AUC: 0.922\n",
      "train_batch 5000/5000 Loss: 5.46049 ACC: 77.380% p_acc: 93.152% AUC: 0.921\n",
      "train Epoch Loss: 5.46049 ACC: 77.380% p_ACC: 93.152% AUC:  0.921\n",
      ".............\n",
      "val_batch 500/833 Loss: 32.87889 ACC: 45.800% p_acc: 75.620% AUC: 0.714\n",
      "val Epoch Loss: 32.45252 ACC: 45.198% p_ACC: 75.400% AUC:  0.710\n",
      ".............\n",
      "-----------------Epoch cost time 260s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 22/500------------\n",
      "train_batch 500/5000 Loss: 4.63440 ACC: 80.133% p_acc: 93.513% AUC: 0.938\n",
      "train_batch 1000/5000 Loss: 4.59507 ACC: 80.300% p_acc: 93.611% AUC: 0.937\n",
      "train_batch 1500/5000 Loss: 4.81421 ACC: 79.322% p_acc: 93.577% AUC: 0.932\n",
      "train_batch 2000/5000 Loss: 4.89950 ACC: 79.025% p_acc: 93.507% AUC: 0.930\n",
      "train_batch 2500/5000 Loss: 4.92349 ACC: 79.000% p_acc: 93.455% AUC: 0.930\n",
      "train_batch 3000/5000 Loss: 5.01980 ACC: 78.761% p_acc: 93.408% AUC: 0.929\n",
      "train_batch 3500/5000 Loss: 5.12234 ACC: 78.381% p_acc: 93.352% AUC: 0.926\n",
      "train_batch 4000/5000 Loss: 5.20071 ACC: 78.275% p_acc: 93.294% AUC: 0.925\n",
      "train_batch 4500/5000 Loss: 5.25368 ACC: 78.167% p_acc: 93.245% AUC: 0.925\n",
      "train_batch 5000/5000 Loss: 5.28739 ACC: 78.163% p_acc: 93.201% AUC: 0.924\n",
      "train Epoch Loss: 5.28739 ACC: 78.163% p_ACC: 93.201% AUC:  0.924\n",
      ".............\n",
      "val_batch 500/833 Loss: 31.84666 ACC: 44.933% p_acc: 74.597% AUC: 0.707\n",
      "val Epoch Loss: 32.91614 ACC: 44.978% p_ACC: 74.799% AUC:  0.705\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 23/500------------\n",
      "train_batch 500/5000 Loss: 4.67829 ACC: 80.233% p_acc: 93.773% AUC: 0.934\n",
      "train_batch 1000/5000 Loss: 4.71256 ACC: 79.650% p_acc: 93.668% AUC: 0.931\n",
      "train_batch 1500/5000 Loss: 4.66597 ACC: 79.500% p_acc: 93.577% AUC: 0.931\n",
      "train_batch 2000/5000 Loss: 4.81232 ACC: 79.308% p_acc: 93.506% AUC: 0.929\n",
      "train_batch 2500/5000 Loss: 4.84473 ACC: 79.087% p_acc: 93.452% AUC: 0.929\n",
      "train_batch 3000/5000 Loss: 4.95479 ACC: 78.767% p_acc: 93.403% AUC: 0.927\n",
      "train_batch 3500/5000 Loss: 4.99296 ACC: 78.524% p_acc: 93.351% AUC: 0.926\n",
      "train_batch 4000/5000 Loss: 5.01061 ACC: 78.417% p_acc: 93.307% AUC: 0.926\n",
      "train_batch 4500/5000 Loss: 5.09042 ACC: 78.148% p_acc: 93.266% AUC: 0.925\n",
      "train_batch 5000/5000 Loss: 5.16040 ACC: 77.970% p_acc: 93.222% AUC: 0.924\n",
      "train Epoch Loss: 5.16040 ACC: 77.970% p_ACC: 93.222% AUC:  0.924\n",
      ".............\n",
      "val_batch 500/833 Loss: 35.32040 ACC: 45.400% p_acc: 74.800% AUC: 0.711\n",
      "val Epoch Loss: 34.65123 ACC: 45.378% p_ACC: 75.008% AUC:  0.715\n",
      ".............\n",
      "-----------------Epoch cost time 262s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 24/500------------\n",
      "train_batch 500/5000 Loss: 4.07817 ACC: 83.067% p_acc: 94.562% AUC: 0.946\n",
      "train_batch 1000/5000 Loss: 4.30554 ACC: 81.417% p_acc: 94.510% AUC: 0.941\n",
      "train_batch 1500/5000 Loss: 4.45530 ACC: 80.811% p_acc: 94.353% AUC: 0.938\n",
      "train_batch 2000/5000 Loss: 4.60698 ACC: 80.408% p_acc: 94.227% AUC: 0.936\n",
      "train_batch 2500/5000 Loss: 4.66529 ACC: 80.060% p_acc: 94.120% AUC: 0.934\n",
      "train_batch 3000/5000 Loss: 4.73412 ACC: 79.839% p_acc: 94.027% AUC: 0.933\n",
      "train_batch 3500/5000 Loss: 4.81824 ACC: 79.495% p_acc: 93.945% AUC: 0.931\n",
      "train_batch 4000/5000 Loss: 4.86855 ACC: 79.446% p_acc: 93.877% AUC: 0.931\n",
      "train_batch 4500/5000 Loss: 4.90865 ACC: 79.256% p_acc: 93.819% AUC: 0.930\n",
      "train_batch 5000/5000 Loss: 4.93419 ACC: 79.080% p_acc: 93.764% AUC: 0.929\n",
      "train Epoch Loss: 4.93419 ACC: 79.080% p_ACC: 93.764% AUC:  0.929\n",
      ".............\n",
      "val_batch 500/833 Loss: 35.27175 ACC: 44.933% p_acc: 75.675% AUC: 0.712\n",
      "val Epoch Loss: 35.48523 ACC: 45.258% p_ACC: 75.455% AUC:  0.713\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 25/500------------\n",
      "train_batch 500/5000 Loss: 4.38317 ACC: 80.633% p_acc: 93.349% AUC: 0.935\n",
      "train_batch 1000/5000 Loss: 4.30469 ACC: 81.383% p_acc: 93.657% AUC: 0.938\n",
      "train_batch 1500/5000 Loss: 4.31779 ACC: 81.233% p_acc: 93.794% AUC: 0.938\n",
      "train_batch 2000/5000 Loss: 4.54474 ACC: 80.833% p_acc: 93.833% AUC: 0.936\n",
      "train_batch 2500/5000 Loss: 4.58712 ACC: 80.613% p_acc: 93.837% AUC: 0.935\n",
      "train_batch 3000/5000 Loss: 4.59458 ACC: 80.511% p_acc: 93.823% AUC: 0.935\n",
      "train_batch 3500/5000 Loss: 4.64971 ACC: 80.276% p_acc: 93.804% AUC: 0.933\n",
      "train_batch 4000/5000 Loss: 4.68521 ACC: 80.008% p_acc: 93.777% AUC: 0.932\n",
      "train_batch 4500/5000 Loss: 4.74540 ACC: 79.904% p_acc: 93.750% AUC: 0.932\n",
      "train_batch 5000/5000 Loss: 4.80395 ACC: 79.643% p_acc: 93.721% AUC: 0.931\n",
      "train Epoch Loss: 4.80395 ACC: 79.643% p_ACC: 93.721% AUC:  0.931\n",
      ".............\n",
      "val_batch 500/833 Loss: 34.06652 ACC: 45.000% p_acc: 75.664% AUC: 0.709\n",
      "val Epoch Loss: 34.42140 ACC: 45.278% p_ACC: 75.468% AUC:  0.708\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 26/500------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_batch 500/5000 Loss: 4.01611 ACC: 81.433% p_acc: 94.148% AUC: 0.942\n",
      "train_batch 1000/5000 Loss: 4.22957 ACC: 81.033% p_acc: 94.150% AUC: 0.940\n",
      "train_batch 1500/5000 Loss: 4.37257 ACC: 80.967% p_acc: 94.116% AUC: 0.939\n",
      "train_batch 2000/5000 Loss: 4.39567 ACC: 80.992% p_acc: 94.087% AUC: 0.939\n",
      "train_batch 2500/5000 Loss: 4.55717 ACC: 80.447% p_acc: 94.045% AUC: 0.936\n",
      "train_batch 3000/5000 Loss: 4.63143 ACC: 80.289% p_acc: 93.993% AUC: 0.935\n",
      "train_batch 3500/5000 Loss: 4.61791 ACC: 80.186% p_acc: 93.949% AUC: 0.935\n",
      "train_batch 4000/5000 Loss: 4.67046 ACC: 80.008% p_acc: 93.908% AUC: 0.934\n",
      "train_batch 4500/5000 Loss: 4.72392 ACC: 79.907% p_acc: 93.869% AUC: 0.933\n",
      "train_batch 5000/5000 Loss: 4.80335 ACC: 79.680% p_acc: 93.830% AUC: 0.932\n",
      "train Epoch Loss: 4.80335 ACC: 79.680% p_ACC: 93.830% AUC:  0.932\n",
      ".............\n",
      "val_batch 500/833 Loss: 33.71514 ACC: 43.900% p_acc: 74.749% AUC: 0.705\n",
      "val Epoch Loss: 34.14974 ACC: 44.858% p_ACC: 74.923% AUC:  0.711\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 27/500------------\n",
      "train_batch 500/5000 Loss: 4.01378 ACC: 81.767% p_acc: 94.225% AUC: 0.943\n",
      "train_batch 1000/5000 Loss: 4.05184 ACC: 82.383% p_acc: 94.393% AUC: 0.945\n",
      "train_batch 1500/5000 Loss: 4.23387 ACC: 81.689% p_acc: 94.398% AUC: 0.942\n",
      "train_batch 2000/5000 Loss: 4.26260 ACC: 81.483% p_acc: 94.366% AUC: 0.941\n",
      "train_batch 2500/5000 Loss: 4.33025 ACC: 81.013% p_acc: 94.324% AUC: 0.939\n",
      "train_batch 3000/5000 Loss: 4.43351 ACC: 80.839% p_acc: 94.269% AUC: 0.937\n",
      "train_batch 3500/5000 Loss: 4.51472 ACC: 80.586% p_acc: 94.210% AUC: 0.936\n",
      "train_batch 4000/5000 Loss: 4.56785 ACC: 80.375% p_acc: 94.156% AUC: 0.935\n",
      "train_batch 4500/5000 Loss: 4.61367 ACC: 80.352% p_acc: 94.108% AUC: 0.935\n",
      "train_batch 5000/5000 Loss: 4.66645 ACC: 80.120% p_acc: 94.065% AUC: 0.934\n",
      "train Epoch Loss: 4.66645 ACC: 80.120% p_ACC: 94.065% AUC:  0.934\n",
      ".............\n",
      "val_batch 500/833 Loss: 34.61551 ACC: 45.933% p_acc: 75.439% AUC: 0.713\n",
      "val Epoch Loss: 35.50431 ACC: 44.618% p_ACC: 75.264% AUC:  0.704\n",
      ".............\n",
      "-----------------Epoch cost time 260s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 28/500------------\n",
      "train_batch 500/5000 Loss: 3.98185 ACC: 83.933% p_acc: 94.848% AUC: 0.949\n",
      "train_batch 1000/5000 Loss: 4.05513 ACC: 83.133% p_acc: 94.822% AUC: 0.946\n",
      "train_batch 1500/5000 Loss: 4.09867 ACC: 82.633% p_acc: 94.760% AUC: 0.944\n",
      "train_batch 2000/5000 Loss: 4.17668 ACC: 82.358% p_acc: 94.688% AUC: 0.943\n",
      "train_batch 2500/5000 Loss: 4.21281 ACC: 82.113% p_acc: 94.631% AUC: 0.942\n",
      "train_batch 3000/5000 Loss: 4.25129 ACC: 81.850% p_acc: 94.579% AUC: 0.941\n",
      "train_batch 3500/5000 Loss: 4.34002 ACC: 81.371% p_acc: 94.519% AUC: 0.939\n",
      "train_batch 4000/5000 Loss: 4.40896 ACC: 81.179% p_acc: 94.462% AUC: 0.938\n",
      "train_batch 4500/5000 Loss: 4.46757 ACC: 81.067% p_acc: 94.408% AUC: 0.938\n",
      "train_batch 5000/5000 Loss: 4.51275 ACC: 80.823% p_acc: 94.358% AUC: 0.937\n",
      "train Epoch Loss: 4.51275 ACC: 80.823% p_ACC: 94.358% AUC:  0.937\n",
      ".............\n",
      "val_batch 500/833 Loss: 33.85510 ACC: 44.533% p_acc: 74.389% AUC: 0.704\n",
      "val Epoch Loss: 34.16733 ACC: 42.777% p_ACC: 74.353% AUC:  0.696\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 29/500------------\n",
      "train_batch 500/5000 Loss: 3.67885 ACC: 83.733% p_acc: 94.502% AUC: 0.949\n",
      "train_batch 1000/5000 Loss: 4.00394 ACC: 82.450% p_acc: 94.598% AUC: 0.944\n",
      "train_batch 1500/5000 Loss: 4.09854 ACC: 81.944% p_acc: 94.538% AUC: 0.943\n",
      "train_batch 2000/5000 Loss: 4.20635 ACC: 81.825% p_acc: 94.496% AUC: 0.943\n",
      "train_batch 2500/5000 Loss: 4.26439 ACC: 81.627% p_acc: 94.452% AUC: 0.941\n",
      "train_batch 3000/5000 Loss: 4.32700 ACC: 81.261% p_acc: 94.404% AUC: 0.940\n",
      "train_batch 3500/5000 Loss: 4.34383 ACC: 81.381% p_acc: 94.366% AUC: 0.940\n",
      "train_batch 4000/5000 Loss: 4.35041 ACC: 81.350% p_acc: 94.337% AUC: 0.940\n",
      "train_batch 4500/5000 Loss: 4.39807 ACC: 81.256% p_acc: 94.309% AUC: 0.939\n",
      "train_batch 5000/5000 Loss: 4.43402 ACC: 81.133% p_acc: 94.281% AUC: 0.938\n",
      "train Epoch Loss: 4.43402 ACC: 81.133% p_ACC: 94.281% AUC:  0.938\n",
      ".............\n",
      "val_batch 500/833 Loss: 34.27130 ACC: 46.000% p_acc: 75.555% AUC: 0.711\n",
      "val Epoch Loss: 35.19077 ACC: 46.559% p_ACC: 75.594% AUC:  0.714\n",
      ".............\n",
      "-----------------Epoch cost time 260s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 30/500------------\n",
      "train_batch 500/5000 Loss: 3.58364 ACC: 84.033% p_acc: 95.294% AUC: 0.951\n",
      "train_batch 1000/5000 Loss: 3.74418 ACC: 83.417% p_acc: 95.200% AUC: 0.948\n",
      "train_batch 1500/5000 Loss: 3.85974 ACC: 83.389% p_acc: 95.076% AUC: 0.948\n",
      "train_batch 2000/5000 Loss: 4.01368 ACC: 82.892% p_acc: 94.992% AUC: 0.945\n",
      "train_batch 2500/5000 Loss: 4.07982 ACC: 82.813% p_acc: 94.921% AUC: 0.945\n",
      "train_batch 3000/5000 Loss: 4.10573 ACC: 82.678% p_acc: 94.864% AUC: 0.944\n",
      "train_batch 3500/5000 Loss: 4.20968 ACC: 82.333% p_acc: 94.806% AUC: 0.943\n",
      "train_batch 4000/5000 Loss: 4.29530 ACC: 81.983% p_acc: 94.750% AUC: 0.941\n",
      "train_batch 4500/5000 Loss: 4.33804 ACC: 81.785% p_acc: 94.694% AUC: 0.940\n",
      "train_batch 5000/5000 Loss: 4.38208 ACC: 81.590% p_acc: 94.643% AUC: 0.940\n",
      "train Epoch Loss: 4.38208 ACC: 81.590% p_ACC: 94.643% AUC:  0.940\n",
      ".............\n",
      "val_batch 500/833 Loss: 35.93525 ACC: 46.967% p_acc: 74.841% AUC: 0.713\n",
      "val Epoch Loss: 36.44681 ACC: 45.938% p_ACC: 74.999% AUC:  0.708\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 31/500------------\n",
      "train_batch 500/5000 Loss: 3.63211 ACC: 82.833% p_acc: 94.823% AUC: 0.947\n",
      "train_batch 1000/5000 Loss: 3.65135 ACC: 83.017% p_acc: 94.840% AUC: 0.946\n",
      "train_batch 1500/5000 Loss: 3.96423 ACC: 82.433% p_acc: 94.758% AUC: 0.942\n",
      "train_batch 2000/5000 Loss: 3.98536 ACC: 82.300% p_acc: 94.680% AUC: 0.942\n",
      "train_batch 2500/5000 Loss: 4.01708 ACC: 82.293% p_acc: 94.632% AUC: 0.942\n",
      "train_batch 3000/5000 Loss: 4.05222 ACC: 82.050% p_acc: 94.593% AUC: 0.941\n",
      "train_batch 3500/5000 Loss: 4.12876 ACC: 81.943% p_acc: 94.555% AUC: 0.941\n",
      "train_batch 4000/5000 Loss: 4.17532 ACC: 81.838% p_acc: 94.519% AUC: 0.940\n",
      "train_batch 4500/5000 Loss: 4.21759 ACC: 81.744% p_acc: 94.487% AUC: 0.940\n",
      "train_batch 5000/5000 Loss: 4.24435 ACC: 81.457% p_acc: 94.455% AUC: 0.939\n",
      "train Epoch Loss: 4.24435 ACC: 81.457% p_ACC: 94.455% AUC:  0.939\n",
      ".............\n",
      "val_batch 500/833 Loss: 35.93814 ACC: 43.300% p_acc: 74.552% AUC: 0.697\n",
      "val Epoch Loss: 36.57281 ACC: 43.637% p_ACC: 74.419% AUC:  0.697\n",
      ".............\n",
      "-----------------Epoch cost time 213s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 32/500------------\n",
      "train_batch 500/5000 Loss: 3.74384 ACC: 84.500% p_acc: 95.165% AUC: 0.951\n",
      "train_batch 1000/5000 Loss: 3.66123 ACC: 84.017% p_acc: 95.154% AUC: 0.951\n",
      "train_batch 1500/5000 Loss: 3.70736 ACC: 83.878% p_acc: 95.146% AUC: 0.950\n",
      "train_batch 2000/5000 Loss: 3.82744 ACC: 83.567% p_acc: 95.095% AUC: 0.948\n",
      "train_batch 2500/5000 Loss: 3.86913 ACC: 83.400% p_acc: 95.054% AUC: 0.948\n",
      "train_batch 3000/5000 Loss: 3.91448 ACC: 83.233% p_acc: 95.013% AUC: 0.947\n",
      "train_batch 3500/5000 Loss: 3.98538 ACC: 82.952% p_acc: 94.974% AUC: 0.946\n",
      "train_batch 4000/5000 Loss: 4.03068 ACC: 82.738% p_acc: 94.934% AUC: 0.946\n",
      "train_batch 4500/5000 Loss: 4.08589 ACC: 82.593% p_acc: 94.896% AUC: 0.945\n",
      "train_batch 5000/5000 Loss: 4.13717 ACC: 82.333% p_acc: 94.855% AUC: 0.943\n",
      "train Epoch Loss: 4.13717 ACC: 82.333% p_ACC: 94.855% AUC:  0.943\n",
      ".............\n",
      "val_batch 500/833 Loss: 35.93628 ACC: 46.200% p_acc: 74.854% AUC: 0.701\n",
      "val Epoch Loss: 35.51038 ACC: 45.218% p_ACC: 74.643% AUC:  0.702\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 33/500------------\n",
      "train_batch 500/5000 Loss: 3.22024 ACC: 85.233% p_acc: 95.363% AUC: 0.956\n",
      "train_batch 1000/5000 Loss: 3.62646 ACC: 85.100% p_acc: 95.408% AUC: 0.953\n",
      "train_batch 1500/5000 Loss: 3.63408 ACC: 84.744% p_acc: 95.368% AUC: 0.952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_batch 2000/5000 Loss: 3.83252 ACC: 84.117% p_acc: 95.296% AUC: 0.948\n",
      "train_batch 2500/5000 Loss: 3.94511 ACC: 83.547% p_acc: 95.213% AUC: 0.946\n",
      "train_batch 3000/5000 Loss: 3.94990 ACC: 83.339% p_acc: 95.134% AUC: 0.946\n",
      "train_batch 3500/5000 Loss: 4.02268 ACC: 83.033% p_acc: 95.069% AUC: 0.945\n",
      "train_batch 4000/5000 Loss: 4.05571 ACC: 82.829% p_acc: 95.007% AUC: 0.944\n",
      "train_batch 4500/5000 Loss: 4.08702 ACC: 82.607% p_acc: 94.954% AUC: 0.943\n",
      "train_batch 5000/5000 Loss: 4.13092 ACC: 82.437% p_acc: 94.903% AUC: 0.942\n",
      "train Epoch Loss: 4.13092 ACC: 82.437% p_ACC: 94.903% AUC:  0.942\n",
      ".............\n",
      "val_batch 500/833 Loss: 38.21800 ACC: 47.333% p_acc: 75.121% AUC: 0.719\n",
      "val Epoch Loss: 38.47224 ACC: 46.899% p_ACC: 75.337% AUC:  0.718\n",
      ".............\n",
      "-----------------Epoch cost time 260s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 34/500------------\n",
      "train_batch 500/5000 Loss: 3.17516 ACC: 85.533% p_acc: 95.500% AUC: 0.958\n",
      "train_batch 1000/5000 Loss: 3.30448 ACC: 85.000% p_acc: 95.529% AUC: 0.955\n",
      "train_batch 1500/5000 Loss: 3.52969 ACC: 84.633% p_acc: 95.465% AUC: 0.953\n",
      "train_batch 2000/5000 Loss: 3.69101 ACC: 83.975% p_acc: 95.394% AUC: 0.951\n",
      "train_batch 2500/5000 Loss: 3.71875 ACC: 83.800% p_acc: 95.329% AUC: 0.950\n",
      "train_batch 3000/5000 Loss: 3.78498 ACC: 83.361% p_acc: 95.265% AUC: 0.948\n",
      "train_batch 3500/5000 Loss: 3.90139 ACC: 83.167% p_acc: 95.200% AUC: 0.947\n",
      "train_batch 4000/5000 Loss: 3.95196 ACC: 83.008% p_acc: 95.140% AUC: 0.946\n",
      "train_batch 4500/5000 Loss: 4.03773 ACC: 82.707% p_acc: 95.084% AUC: 0.944\n",
      "train_batch 5000/5000 Loss: 4.10553 ACC: 82.347% p_acc: 95.029% AUC: 0.943\n",
      "train Epoch Loss: 4.10553 ACC: 82.347% p_ACC: 95.029% AUC:  0.943\n",
      ".............\n",
      "val_batch 500/833 Loss: 33.47728 ACC: 44.733% p_acc: 74.485% AUC: 0.703\n",
      "val Epoch Loss: 33.56334 ACC: 45.338% p_ACC: 74.648% AUC:  0.705\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 35/500------------\n",
      "train_batch 500/5000 Loss: 3.43130 ACC: 86.233% p_acc: 95.698% AUC: 0.959\n",
      "train_batch 1000/5000 Loss: 3.49149 ACC: 85.167% p_acc: 95.636% AUC: 0.955\n",
      "train_batch 1500/5000 Loss: 3.52672 ACC: 84.889% p_acc: 95.583% AUC: 0.954\n",
      "train_batch 2000/5000 Loss: 3.54055 ACC: 84.708% p_acc: 95.530% AUC: 0.953\n",
      "train_batch 2500/5000 Loss: 3.63696 ACC: 84.253% p_acc: 95.466% AUC: 0.951\n",
      "train_batch 3000/5000 Loss: 3.75984 ACC: 83.872% p_acc: 95.402% AUC: 0.950\n",
      "train_batch 3500/5000 Loss: 3.83301 ACC: 83.633% p_acc: 95.339% AUC: 0.949\n",
      "train_batch 4000/5000 Loss: 3.92315 ACC: 83.321% p_acc: 95.278% AUC: 0.947\n",
      "train_batch 4500/5000 Loss: 3.93677 ACC: 83.144% p_acc: 95.224% AUC: 0.947\n",
      "train_batch 5000/5000 Loss: 3.96989 ACC: 83.060% p_acc: 95.174% AUC: 0.946\n",
      "train Epoch Loss: 3.96989 ACC: 83.060% p_ACC: 95.174% AUC:  0.946\n",
      ".............\n",
      "val_batch 500/833 Loss: 40.05859 ACC: 43.533% p_acc: 74.797% AUC: 0.699\n",
      "val Epoch Loss: 39.32178 ACC: 43.978% p_ACC: 74.680% AUC:  0.703\n",
      ".............\n",
      "-----------------Epoch cost time 262s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 36/500------------\n",
      "train_batch 500/5000 Loss: 3.22829 ACC: 85.333% p_acc: 95.659% AUC: 0.957\n",
      "train_batch 1000/5000 Loss: 3.44021 ACC: 84.950% p_acc: 95.521% AUC: 0.954\n",
      "train_batch 1500/5000 Loss: 3.52565 ACC: 84.522% p_acc: 95.442% AUC: 0.952\n",
      "train_batch 2000/5000 Loss: 3.59846 ACC: 84.158% p_acc: 95.373% AUC: 0.951\n",
      "train_batch 2500/5000 Loss: 3.73293 ACC: 83.920% p_acc: 95.303% AUC: 0.949\n",
      "train_batch 3000/5000 Loss: 3.80082 ACC: 83.389% p_acc: 95.234% AUC: 0.948\n",
      "train_batch 3500/5000 Loss: 3.82838 ACC: 83.310% p_acc: 95.175% AUC: 0.947\n",
      "train_batch 4000/5000 Loss: 3.90084 ACC: 83.158% p_acc: 95.120% AUC: 0.946\n",
      "train_batch 4500/5000 Loss: 3.92912 ACC: 83.007% p_acc: 95.072% AUC: 0.945\n",
      "train_batch 5000/5000 Loss: 3.96279 ACC: 82.837% p_acc: 95.026% AUC: 0.945\n",
      "train Epoch Loss: 3.96279 ACC: 82.837% p_ACC: 95.026% AUC:  0.945\n",
      ".............\n",
      "val_batch 500/833 Loss: 36.71904 ACC: 46.100% p_acc: 75.426% AUC: 0.710\n",
      "val Epoch Loss: 36.78981 ACC: 46.038% p_ACC: 75.268% AUC:  0.710\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 37/500------------\n",
      "train_batch 500/5000 Loss: 3.17225 ACC: 86.333% p_acc: 95.537% AUC: 0.960\n",
      "train_batch 1000/5000 Loss: 3.33230 ACC: 85.567% p_acc: 95.655% AUC: 0.957\n",
      "train_batch 1500/5000 Loss: 3.45938 ACC: 85.133% p_acc: 95.634% AUC: 0.955\n",
      "train_batch 2000/5000 Loss: 3.54343 ACC: 84.858% p_acc: 95.575% AUC: 0.954\n",
      "train_batch 2500/5000 Loss: 3.57029 ACC: 84.547% p_acc: 95.528% AUC: 0.953\n",
      "train_batch 3000/5000 Loss: 3.61914 ACC: 84.250% p_acc: 95.478% AUC: 0.951\n",
      "train_batch 3500/5000 Loss: 3.65294 ACC: 83.990% p_acc: 95.427% AUC: 0.950\n",
      "train_batch 4000/5000 Loss: 3.70961 ACC: 83.883% p_acc: 95.380% AUC: 0.949\n",
      "train_batch 4500/5000 Loss: 3.77509 ACC: 83.681% p_acc: 95.335% AUC: 0.948\n",
      "train_batch 5000/5000 Loss: 3.84318 ACC: 83.407% p_acc: 95.290% AUC: 0.947\n",
      "train Epoch Loss: 3.84318 ACC: 83.407% p_ACC: 95.290% AUC:  0.947\n",
      ".............\n",
      "val_batch 500/833 Loss: 34.31701 ACC: 44.433% p_acc: 74.471% AUC: 0.699\n",
      "val Epoch Loss: 34.24768 ACC: 45.258% p_ACC: 74.505% AUC:  0.703\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 38/500------------\n",
      "train_batch 500/5000 Loss: 3.38614 ACC: 85.967% p_acc: 95.801% AUC: 0.956\n",
      "train_batch 1000/5000 Loss: 3.44149 ACC: 85.567% p_acc: 95.726% AUC: 0.956\n",
      "train_batch 1500/5000 Loss: 3.51623 ACC: 84.967% p_acc: 95.659% AUC: 0.955\n",
      "train_batch 2000/5000 Loss: 3.57217 ACC: 84.442% p_acc: 95.588% AUC: 0.953\n",
      "train_batch 2500/5000 Loss: 3.69041 ACC: 84.013% p_acc: 95.499% AUC: 0.951\n",
      "train_batch 3000/5000 Loss: 3.77928 ACC: 83.722% p_acc: 95.416% AUC: 0.949\n",
      "train_batch 3500/5000 Loss: 3.79469 ACC: 83.614% p_acc: 95.349% AUC: 0.949\n",
      "train_batch 4000/5000 Loss: 3.82756 ACC: 83.363% p_acc: 95.291% AUC: 0.947\n",
      "train_batch 4500/5000 Loss: 3.88690 ACC: 83.167% p_acc: 95.237% AUC: 0.947\n",
      "train_batch 5000/5000 Loss: 3.91765 ACC: 83.040% p_acc: 95.189% AUC: 0.946\n",
      "train Epoch Loss: 3.91765 ACC: 83.040% p_ACC: 95.189% AUC:  0.946\n",
      ".............\n",
      "val_batch 500/833 Loss: 38.95409 ACC: 46.533% p_acc: 74.893% AUC: 0.715\n",
      "val Epoch Loss: 38.53415 ACC: 46.539% p_ACC: 75.124% AUC:  0.715\n",
      ".............\n",
      "-----------------Epoch cost time 262s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 39/500------------\n",
      "train_batch 500/5000 Loss: 3.11079 ACC: 86.467% p_acc: 96.112% AUC: 0.959\n",
      "train_batch 1000/5000 Loss: 3.24283 ACC: 85.567% p_acc: 95.929% AUC: 0.956\n",
      "train_batch 1500/5000 Loss: 3.29637 ACC: 85.433% p_acc: 95.813% AUC: 0.956\n",
      "train_batch 2000/5000 Loss: 3.41068 ACC: 84.800% p_acc: 95.712% AUC: 0.953\n",
      "train_batch 2500/5000 Loss: 3.40411 ACC: 84.767% p_acc: 95.636% AUC: 0.953\n",
      "train_batch 3000/5000 Loss: 3.53125 ACC: 84.289% p_acc: 95.572% AUC: 0.951\n",
      "train_batch 3500/5000 Loss: 3.56929 ACC: 84.119% p_acc: 95.510% AUC: 0.950\n",
      "train_batch 4000/5000 Loss: 3.63849 ACC: 83.875% p_acc: 95.456% AUC: 0.949\n",
      "train_batch 4500/5000 Loss: 3.69754 ACC: 83.641% p_acc: 95.404% AUC: 0.948\n",
      "train_batch 5000/5000 Loss: 3.77401 ACC: 83.477% p_acc: 95.354% AUC: 0.947\n",
      "train Epoch Loss: 3.77401 ACC: 83.477% p_ACC: 95.354% AUC:  0.947\n",
      ".............\n",
      "val_batch 500/833 Loss: 35.25008 ACC: 43.700% p_acc: 74.138% AUC: 0.692\n",
      "val Epoch Loss: 35.17359 ACC: 44.018% p_ACC: 74.148% AUC:  0.694\n",
      ".............\n",
      "-----------------Epoch cost time 262s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 40/500------------\n",
      "train_batch 500/5000 Loss: 3.25944 ACC: 85.033% p_acc: 95.649% AUC: 0.956\n",
      "train_batch 1000/5000 Loss: 3.31024 ACC: 84.833% p_acc: 95.578% AUC: 0.955\n",
      "train_batch 1500/5000 Loss: 3.38811 ACC: 84.600% p_acc: 95.515% AUC: 0.953\n",
      "train_batch 2000/5000 Loss: 3.49642 ACC: 84.383% p_acc: 95.462% AUC: 0.952\n",
      "train_batch 2500/5000 Loss: 3.59416 ACC: 83.953% p_acc: 95.403% AUC: 0.951\n",
      "train_batch 3000/5000 Loss: 3.58546 ACC: 83.956% p_acc: 95.356% AUC: 0.950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_batch 3500/5000 Loss: 3.63817 ACC: 83.805% p_acc: 95.315% AUC: 0.950\n",
      "train_batch 4000/5000 Loss: 3.69063 ACC: 83.700% p_acc: 95.280% AUC: 0.949\n",
      "train_batch 4500/5000 Loss: 3.70267 ACC: 83.696% p_acc: 95.249% AUC: 0.949\n",
      "train_batch 5000/5000 Loss: 3.75723 ACC: 83.617% p_acc: 95.220% AUC: 0.948\n",
      "train Epoch Loss: 3.75723 ACC: 83.617% p_ACC: 95.220% AUC:  0.948\n",
      ".............\n",
      "val_batch 500/833 Loss: 35.50766 ACC: 45.133% p_acc: 74.379% AUC: 0.698\n",
      "val Epoch Loss: 35.33614 ACC: 44.738% p_ACC: 74.340% AUC:  0.697\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 41/500------------\n",
      "train_batch 500/5000 Loss: 3.37353 ACC: 85.867% p_acc: 95.625% AUC: 0.959\n",
      "train_batch 1000/5000 Loss: 3.36283 ACC: 85.600% p_acc: 95.686% AUC: 0.957\n",
      "train_batch 1500/5000 Loss: 3.44874 ACC: 85.178% p_acc: 95.649% AUC: 0.955\n",
      "train_batch 2000/5000 Loss: 3.43909 ACC: 84.825% p_acc: 95.605% AUC: 0.954\n",
      "train_batch 2500/5000 Loss: 3.50216 ACC: 84.827% p_acc: 95.559% AUC: 0.954\n",
      "train_batch 3000/5000 Loss: 3.55979 ACC: 84.600% p_acc: 95.523% AUC: 0.952\n",
      "train_batch 3500/5000 Loss: 3.59162 ACC: 84.371% p_acc: 95.484% AUC: 0.952\n",
      "train_batch 4000/5000 Loss: 3.59493 ACC: 84.288% p_acc: 95.450% AUC: 0.951\n",
      "train_batch 4500/5000 Loss: 3.63063 ACC: 84.100% p_acc: 95.416% AUC: 0.950\n",
      "train_batch 5000/5000 Loss: 3.67028 ACC: 84.000% p_acc: 95.384% AUC: 0.950\n",
      "train Epoch Loss: 3.67028 ACC: 84.000% p_ACC: 95.384% AUC:  0.950\n",
      ".............\n",
      "val_batch 500/833 Loss: 38.18191 ACC: 46.167% p_acc: 74.816% AUC: 0.712\n",
      "val Epoch Loss: 39.32087 ACC: 45.438% p_ACC: 74.929% AUC:  0.708\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 42/500------------\n",
      "train_batch 500/5000 Loss: 2.95985 ACC: 87.400% p_acc: 96.295% AUC: 0.963\n",
      "train_batch 1000/5000 Loss: 3.16283 ACC: 85.983% p_acc: 96.114% AUC: 0.959\n",
      "train_batch 1500/5000 Loss: 3.24720 ACC: 85.978% p_acc: 95.993% AUC: 0.958\n",
      "train_batch 2000/5000 Loss: 3.27098 ACC: 85.642% p_acc: 95.912% AUC: 0.957\n",
      "train_batch 2500/5000 Loss: 3.41476 ACC: 85.340% p_acc: 95.838% AUC: 0.955\n",
      "train_batch 3000/5000 Loss: 3.45454 ACC: 84.933% p_acc: 95.760% AUC: 0.953\n",
      "train_batch 3500/5000 Loss: 3.52241 ACC: 84.776% p_acc: 95.697% AUC: 0.952\n",
      "train_batch 4000/5000 Loss: 3.59074 ACC: 84.583% p_acc: 95.640% AUC: 0.951\n",
      "train_batch 4500/5000 Loss: 3.66435 ACC: 84.359% p_acc: 95.586% AUC: 0.950\n",
      "train_batch 5000/5000 Loss: 3.74232 ACC: 84.120% p_acc: 95.531% AUC: 0.949\n",
      "train Epoch Loss: 3.74232 ACC: 84.120% p_ACC: 95.531% AUC:  0.949\n",
      ".............\n",
      "val_batch 500/833 Loss: 35.45232 ACC: 44.967% p_acc: 75.459% AUC: 0.703\n",
      "val Epoch Loss: 35.61881 ACC: 45.198% p_ACC: 75.296% AUC:  0.708\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 43/500------------\n",
      "train_batch 500/5000 Loss: 3.19502 ACC: 86.433% p_acc: 95.477% AUC: 0.955\n",
      "train_batch 1000/5000 Loss: 3.25975 ACC: 85.667% p_acc: 95.563% AUC: 0.955\n",
      "train_batch 1500/5000 Loss: 3.34527 ACC: 85.456% p_acc: 95.554% AUC: 0.954\n",
      "train_batch 2000/5000 Loss: 3.41620 ACC: 85.258% p_acc: 95.513% AUC: 0.953\n",
      "train_batch 2500/5000 Loss: 3.46948 ACC: 84.913% p_acc: 95.484% AUC: 0.952\n",
      "train_batch 3000/5000 Loss: 3.52609 ACC: 84.872% p_acc: 95.453% AUC: 0.952\n",
      "train_batch 3500/5000 Loss: 3.53644 ACC: 84.876% p_acc: 95.434% AUC: 0.952\n",
      "train_batch 4000/5000 Loss: 3.61098 ACC: 84.604% p_acc: 95.414% AUC: 0.951\n",
      "train_batch 4500/5000 Loss: 3.65541 ACC: 84.463% p_acc: 95.391% AUC: 0.951\n",
      "train_batch 5000/5000 Loss: 3.70231 ACC: 84.267% p_acc: 95.366% AUC: 0.950\n",
      "train Epoch Loss: 3.70231 ACC: 84.267% p_ACC: 95.366% AUC:  0.950\n",
      ".............\n",
      "val_batch 500/833 Loss: 36.61615 ACC: 45.300% p_acc: 75.324% AUC: 0.704\n",
      "val Epoch Loss: 36.37202 ACC: 45.458% p_ACC: 75.133% AUC:  0.706\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 44/500------------\n",
      "train_batch 500/5000 Loss: 2.89369 ACC: 86.967% p_acc: 96.347% AUC: 0.963\n",
      "train_batch 1000/5000 Loss: 3.00543 ACC: 86.483% p_acc: 96.220% AUC: 0.960\n",
      "train_batch 1500/5000 Loss: 3.11813 ACC: 86.122% p_acc: 96.103% AUC: 0.958\n",
      "train_batch 3500/5000 Loss: 3.32602 ACC: 85.238% p_acc: 95.823% AUC: 0.954\n",
      "train_batch 4000/5000 Loss: 3.35113 ACC: 85.171% p_acc: 95.776% AUC: 0.953\n",
      "train_batch 4500/5000 Loss: 3.44058 ACC: 84.896% p_acc: 95.732% AUC: 0.952\n",
      "train_batch 5000/5000 Loss: 3.50978 ACC: 84.717% p_acc: 95.687% AUC: 0.951\n",
      "train Epoch Loss: 3.50978 ACC: 84.717% p_ACC: 95.687% AUC:  0.951\n",
      ".............\n",
      "val_batch 500/833 Loss: 37.19008 ACC: 44.533% p_acc: 74.866% AUC: 0.700\n",
      "val Epoch Loss: 36.67583 ACC: 44.438% p_ACC: 74.722% AUC:  0.705\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 45/500------------\n",
      "train_batch 500/5000 Loss: 2.96558 ACC: 87.067% p_acc: 96.131% AUC: 0.963\n",
      "train_batch 1000/5000 Loss: 3.09149 ACC: 86.567% p_acc: 96.102% AUC: 0.960\n",
      "train_batch 1500/5000 Loss: 3.12287 ACC: 86.189% p_acc: 96.033% AUC: 0.959\n",
      "train_batch 2000/5000 Loss: 3.16034 ACC: 85.967% p_acc: 95.977% AUC: 0.958\n",
      "train_batch 2500/5000 Loss: 3.25129 ACC: 85.733% p_acc: 95.929% AUC: 0.957\n",
      "train_batch 3000/5000 Loss: 3.27635 ACC: 85.572% p_acc: 95.882% AUC: 0.956\n",
      "train_batch 3500/5000 Loss: 3.37856 ACC: 85.300% p_acc: 95.836% AUC: 0.955\n",
      "train_batch 4000/5000 Loss: 3.44844 ACC: 85.100% p_acc: 95.791% AUC: 0.954\n",
      "train_batch 4500/5000 Loss: 3.52437 ACC: 84.885% p_acc: 95.747% AUC: 0.953\n",
      "train_batch 5000/5000 Loss: 3.56995 ACC: 84.797% p_acc: 95.706% AUC: 0.953\n",
      "train Epoch Loss: 3.56995 ACC: 84.797% p_ACC: 95.706% AUC:  0.953\n",
      ".............\n",
      "val_batch 500/833 Loss: 36.84456 ACC: 41.833% p_acc: 73.304% AUC: 0.687\n",
      "val Epoch Loss: 36.19820 ACC: 43.778% p_ACC: 73.608% AUC:  0.697\n",
      ".............\n",
      "-----------------Epoch cost time 262s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 46/500------------\n",
      "train_batch 500/5000 Loss: 2.64624 ACC: 88.167% p_acc: 96.494% AUC: 0.966\n",
      "train_batch 1000/5000 Loss: 2.86703 ACC: 87.483% p_acc: 96.455% AUC: 0.962\n",
      "train_batch 1500/5000 Loss: 3.02381 ACC: 87.033% p_acc: 96.350% AUC: 0.961\n",
      "train_batch 2000/5000 Loss: 3.09755 ACC: 86.867% p_acc: 96.269% AUC: 0.960\n",
      "train_batch 2500/5000 Loss: 3.20908 ACC: 86.467% p_acc: 96.200% AUC: 0.958\n",
      "train_batch 3000/5000 Loss: 3.31838 ACC: 85.944% p_acc: 96.127% AUC: 0.956\n",
      "train_batch 3500/5000 Loss: 3.36904 ACC: 85.767% p_acc: 96.058% AUC: 0.955\n",
      "train_batch 4000/5000 Loss: 3.42651 ACC: 85.529% p_acc: 95.993% AUC: 0.954\n",
      "train_batch 4500/5000 Loss: 3.44599 ACC: 85.444% p_acc: 95.937% AUC: 0.954\n",
      "train_batch 5000/5000 Loss: 3.52208 ACC: 85.183% p_acc: 95.886% AUC: 0.953\n",
      "train Epoch Loss: 3.52208 ACC: 85.183% p_ACC: 95.886% AUC:  0.953\n",
      ".............\n",
      "val_batch 500/833 Loss: 38.13340 ACC: 43.933% p_acc: 75.134% AUC: 0.694\n",
      "val Epoch Loss: 37.47680 ACC: 44.298% p_ACC: 74.695% AUC:  0.697\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 47/500------------\n",
      "train_batch 500/5000 Loss: 2.78633 ACC: 87.900% p_acc: 96.781% AUC: 0.964\n",
      "train_batch 1000/5000 Loss: 2.90173 ACC: 87.150% p_acc: 96.524% AUC: 0.961\n",
      "train_batch 1500/5000 Loss: 3.06030 ACC: 86.656% p_acc: 96.344% AUC: 0.959\n",
      "train_batch 2000/5000 Loss: 3.12864 ACC: 86.300% p_acc: 96.223% AUC: 0.958\n",
      "train_batch 2500/5000 Loss: 3.21872 ACC: 86.047% p_acc: 96.133% AUC: 0.957\n",
      "train_batch 3000/5000 Loss: 3.24994 ACC: 86.006% p_acc: 96.064% AUC: 0.957\n",
      "train_batch 3500/5000 Loss: 3.28599 ACC: 85.895% p_acc: 96.013% AUC: 0.956\n",
      "train_batch 4000/5000 Loss: 3.38177 ACC: 85.563% p_acc: 95.962% AUC: 0.954\n",
      "train_batch 4500/5000 Loss: 3.42585 ACC: 85.348% p_acc: 95.913% AUC: 0.954\n",
      "train_batch 5000/5000 Loss: 3.47612 ACC: 85.107% p_acc: 95.866% AUC: 0.953\n",
      "train Epoch Loss: 3.47612 ACC: 85.107% p_ACC: 95.866% AUC:  0.953\n",
      ".............\n",
      "val_batch 500/833 Loss: 37.87442 ACC: 47.400% p_acc: 76.133% AUC: 0.715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val Epoch Loss: 37.87540 ACC: 46.198% p_ACC: 75.969% AUC:  0.712\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 48/500------------\n",
      "train_batch 500/5000 Loss: 2.98442 ACC: 87.500% p_acc: 96.202% AUC: 0.960\n",
      "train_batch 1000/5000 Loss: 3.00527 ACC: 86.967% p_acc: 96.139% AUC: 0.960\n",
      "train_batch 1500/5000 Loss: 3.12284 ACC: 86.511% p_acc: 96.091% AUC: 0.958\n",
      "train_batch 2000/5000 Loss: 3.15126 ACC: 86.217% p_acc: 96.034% AUC: 0.957\n",
      "train_batch 2500/5000 Loss: 3.18860 ACC: 86.027% p_acc: 95.981% AUC: 0.956\n",
      "train_batch 3000/5000 Loss: 3.30264 ACC: 85.767% p_acc: 95.931% AUC: 0.955\n",
      "train_batch 3500/5000 Loss: 3.33876 ACC: 85.448% p_acc: 95.877% AUC: 0.954\n",
      "train_batch 4000/5000 Loss: 3.39290 ACC: 85.329% p_acc: 95.829% AUC: 0.953\n",
      "train_batch 4500/5000 Loss: 3.43107 ACC: 85.163% p_acc: 95.786% AUC: 0.953\n",
      "train_batch 5000/5000 Loss: 3.45257 ACC: 85.070% p_acc: 95.748% AUC: 0.952\n",
      "train Epoch Loss: 3.45257 ACC: 85.070% p_ACC: 95.748% AUC:  0.952\n",
      ".............\n",
      "val_batch 500/833 Loss: 44.09955 ACC: 47.367% p_acc: 76.694% AUC: 0.722\n",
      "val Epoch Loss: 44.21944 ACC: 46.899% p_ACC: 76.485% AUC:  0.718\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 49/500------------\n",
      "train_batch 500/5000 Loss: 2.81100 ACC: 88.033% p_acc: 96.219% AUC: 0.963\n",
      "train_batch 1000/5000 Loss: 2.89574 ACC: 87.500% p_acc: 96.233% AUC: 0.961\n",
      "train_batch 1500/5000 Loss: 3.06108 ACC: 87.011% p_acc: 96.185% AUC: 0.960\n",
      "train_batch 3500/5000 Loss: 3.31797 ACC: 85.752% p_acc: 95.961% AUC: 0.954\n",
      "train_batch 4000/5000 Loss: 3.35708 ACC: 85.546% p_acc: 95.909% AUC: 0.954\n",
      "train_batch 4500/5000 Loss: 3.35934 ACC: 85.378% p_acc: 95.861% AUC: 0.953\n",
      "train_batch 5000/5000 Loss: 3.40718 ACC: 85.200% p_acc: 95.817% AUC: 0.953\n",
      "train Epoch Loss: 3.40718 ACC: 85.200% p_ACC: 95.817% AUC:  0.953\n",
      ".............\n",
      "val_batch 500/833 Loss: 39.40069 ACC: 44.933% p_acc: 75.109% AUC: 0.702\n",
      "val Epoch Loss: 39.20822 ACC: 45.418% p_ACC: 74.915% AUC:  0.705\n",
      ".............\n",
      "-----------------Epoch cost time 262s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 50/500------------\n",
      "train_batch 500/5000 Loss: 2.94314 ACC: 86.500% p_acc: 96.205% AUC: 0.960\n",
      "train_batch 1000/5000 Loss: 2.96819 ACC: 86.667% p_acc: 96.109% AUC: 0.960\n",
      "train_batch 1500/5000 Loss: 3.01085 ACC: 86.500% p_acc: 96.078% AUC: 0.960\n",
      "train_batch 2000/5000 Loss: 3.14087 ACC: 86.083% p_acc: 96.026% AUC: 0.957\n",
      "train_batch 2500/5000 Loss: 3.20308 ACC: 85.913% p_acc: 95.966% AUC: 0.957\n",
      "train_batch 3000/5000 Loss: 3.24220 ACC: 85.950% p_acc: 95.917% AUC: 0.956\n",
      "train_batch 3500/5000 Loss: 3.32980 ACC: 85.743% p_acc: 95.877% AUC: 0.955\n",
      "train_batch 4000/5000 Loss: 3.34738 ACC: 85.654% p_acc: 95.838% AUC: 0.955\n",
      "train_batch 4500/5000 Loss: 3.40907 ACC: 85.441% p_acc: 95.805% AUC: 0.954\n",
      "train_batch 5000/5000 Loss: 3.44185 ACC: 85.323% p_acc: 95.770% AUC: 0.953\n",
      "train Epoch Loss: 3.44185 ACC: 85.323% p_ACC: 95.770% AUC:  0.953\n",
      ".............\n",
      "val_batch 500/833 Loss: 37.89103 ACC: 43.267% p_acc: 74.246% AUC: 0.691\n",
      "val Epoch Loss: 37.34012 ACC: 43.497% p_ACC: 73.965% AUC:  0.692\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 51/500------------\n",
      "train_batch 500/5000 Loss: 2.96857 ACC: 87.133% p_acc: 96.134% AUC: 0.961\n",
      "train_batch 1000/5000 Loss: 2.89634 ACC: 87.300% p_acc: 96.139% AUC: 0.961\n",
      "train_batch 1500/5000 Loss: 3.04357 ACC: 86.722% p_acc: 96.104% AUC: 0.959\n",
      "train_batch 2000/5000 Loss: 3.03871 ACC: 86.833% p_acc: 96.074% AUC: 0.960\n",
      "train_batch 2500/5000 Loss: 3.08956 ACC: 86.713% p_acc: 96.060% AUC: 0.960\n",
      "train_batch 3000/5000 Loss: 3.10660 ACC: 86.478% p_acc: 96.038% AUC: 0.959\n",
      "train_batch 3500/5000 Loss: 3.17238 ACC: 86.386% p_acc: 96.011% AUC: 0.958\n",
      "train_batch 4000/5000 Loss: 3.26836 ACC: 86.067% p_acc: 95.979% AUC: 0.956\n",
      "train_batch 4500/5000 Loss: 3.29812 ACC: 85.922% p_acc: 95.948% AUC: 0.956\n",
      "train_batch 5000/5000 Loss: 3.35162 ACC: 85.687% p_acc: 95.915% AUC: 0.955\n",
      "train Epoch Loss: 3.35162 ACC: 85.687% p_ACC: 95.915% AUC:  0.955\n",
      ".............\n",
      "val_batch 500/833 Loss: 38.57875 ACC: 45.633% p_acc: 74.882% AUC: 0.705\n",
      "val Epoch Loss: 38.82842 ACC: 45.718% p_ACC: 74.990% AUC:  0.705\n",
      ".............\n",
      "-----------------Epoch cost time 262s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 52/500------------\n",
      "train_batch 500/5000 Loss: 2.71722 ACC: 87.633% p_acc: 96.712% AUC: 0.964\n",
      "train_batch 1000/5000 Loss: 2.77209 ACC: 87.667% p_acc: 96.546% AUC: 0.964\n",
      "train_batch 1500/5000 Loss: 2.86490 ACC: 87.256% p_acc: 96.452% AUC: 0.962\n",
      "train_batch 2000/5000 Loss: 2.95544 ACC: 86.950% p_acc: 96.378% AUC: 0.961\n",
      "train_batch 2500/5000 Loss: 3.01108 ACC: 86.860% p_acc: 96.321% AUC: 0.960\n",
      "train_batch 3000/5000 Loss: 3.07412 ACC: 86.461% p_acc: 96.265% AUC: 0.959\n",
      "train_batch 3500/5000 Loss: 3.19759 ACC: 86.257% p_acc: 96.207% AUC: 0.957\n",
      "train_batch 4000/5000 Loss: 3.25225 ACC: 86.067% p_acc: 96.153% AUC: 0.956\n",
      "train_batch 4500/5000 Loss: 3.32269 ACC: 85.900% p_acc: 96.102% AUC: 0.955\n",
      "train_batch 5000/5000 Loss: 3.33336 ACC: 85.830% p_acc: 96.057% AUC: 0.955\n",
      "train Epoch Loss: 3.33336 ACC: 85.830% p_ACC: 96.057% AUC:  0.955\n",
      ".............\n",
      "val_batch 500/833 Loss: 38.01826 ACC: 44.867% p_acc: 75.070% AUC: 0.710\n",
      "val Epoch Loss: 39.30755 ACC: 43.677% p_ACC: 74.883% AUC:  0.701\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 53/500------------\n",
      "train_batch 500/5000 Loss: 3.14541 ACC: 86.800% p_acc: 95.879% AUC: 0.961\n",
      "train_batch 1000/5000 Loss: 2.92182 ACC: 87.417% p_acc: 96.010% AUC: 0.963\n",
      "train_batch 1500/5000 Loss: 3.04495 ACC: 86.911% p_acc: 96.067% AUC: 0.960\n",
      "train_batch 2000/5000 Loss: 3.08939 ACC: 86.650% p_acc: 96.051% AUC: 0.959\n",
      "train_batch 2500/5000 Loss: 3.16528 ACC: 86.380% p_acc: 96.032% AUC: 0.958\n",
      "train_batch 3000/5000 Loss: 3.20736 ACC: 86.144% p_acc: 96.001% AUC: 0.957\n",
      "train_batch 3500/5000 Loss: 3.22987 ACC: 85.929% p_acc: 95.966% AUC: 0.956\n",
      "train_batch 4000/5000 Loss: 3.30638 ACC: 85.688% p_acc: 95.926% AUC: 0.955\n",
      "train_batch 4500/5000 Loss: 3.33494 ACC: 85.600% p_acc: 95.889% AUC: 0.955\n",
      "train_batch 5000/5000 Loss: 3.38081 ACC: 85.407% p_acc: 95.854% AUC: 0.954\n",
      "train Epoch Loss: 3.38081 ACC: 85.407% p_ACC: 95.854% AUC:  0.954\n",
      ".............\n",
      "val_batch 500/833 Loss: 39.21300 ACC: 45.567% p_acc: 75.127% AUC: 0.714\n",
      "val Epoch Loss: 39.59866 ACC: 45.198% p_ACC: 75.181% AUC:  0.712\n",
      ".............\n",
      "-----------------Epoch cost time 262s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 54/500------------\n",
      "train_batch 500/5000 Loss: 2.54642 ACC: 88.867% p_acc: 96.707% AUC: 0.967\n",
      "train_batch 1000/5000 Loss: 2.76662 ACC: 87.750% p_acc: 96.556% AUC: 0.962\n",
      "train_batch 1500/5000 Loss: 2.86265 ACC: 87.300% p_acc: 96.461% AUC: 0.961\n",
      "train_batch 2000/5000 Loss: 2.96860 ACC: 87.283% p_acc: 96.377% AUC: 0.961\n",
      "train_batch 2500/5000 Loss: 3.06102 ACC: 87.153% p_acc: 96.323% AUC: 0.960\n",
      "train_batch 3000/5000 Loss: 3.11513 ACC: 86.933% p_acc: 96.273% AUC: 0.959\n",
      "train_batch 3500/5000 Loss: 3.16689 ACC: 86.733% p_acc: 96.222% AUC: 0.958\n",
      "train_batch 4000/5000 Loss: 3.20643 ACC: 86.608% p_acc: 96.181% AUC: 0.958\n",
      "train_batch 4500/5000 Loss: 3.23012 ACC: 86.385% p_acc: 96.142% AUC: 0.957\n",
      "train_batch 5000/5000 Loss: 3.30367 ACC: 86.133% p_acc: 96.102% AUC: 0.956\n",
      "train Epoch Loss: 3.30367 ACC: 86.133% p_ACC: 96.102% AUC:  0.956\n",
      ".............\n",
      "val_batch 500/833 Loss: 35.92272 ACC: 44.400% p_acc: 75.362% AUC: 0.703\n",
      "val Epoch Loss: 36.60730 ACC: 45.218% p_ACC: 75.118% AUC:  0.707\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 55/500------------\n",
      "train_batch 500/5000 Loss: 2.74827 ACC: 87.933% p_acc: 96.608% AUC: 0.964\n",
      "train_batch 1000/5000 Loss: 2.77388 ACC: 87.717% p_acc: 96.440% AUC: 0.963\n",
      "train_batch 1500/5000 Loss: 2.89112 ACC: 87.244% p_acc: 96.383% AUC: 0.961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_batch 2000/5000 Loss: 2.97561 ACC: 87.125% p_acc: 96.311% AUC: 0.960\n",
      "train_batch 2500/5000 Loss: 3.00451 ACC: 86.900% p_acc: 96.246% AUC: 0.959\n",
      "train_batch 3000/5000 Loss: 3.00723 ACC: 86.711% p_acc: 96.195% AUC: 0.959\n",
      "train_batch 3500/5000 Loss: 3.04925 ACC: 86.433% p_acc: 96.151% AUC: 0.958\n",
      "train_batch 4000/5000 Loss: 3.17215 ACC: 86.146% p_acc: 96.107% AUC: 0.957\n",
      "train_batch 1000/5000 Loss: 2.79140 ACC: 88.333% p_acc: 96.857% AUC: 0.966\n",
      "train_batch 1500/5000 Loss: 2.96097 ACC: 87.667% p_acc: 96.679% AUC: 0.962\n",
      "train_batch 2000/5000 Loss: 3.05107 ACC: 87.242% p_acc: 96.548% AUC: 0.960\n",
      "train_batch 2500/5000 Loss: 3.10126 ACC: 87.047% p_acc: 96.448% AUC: 0.959\n",
      "train_batch 3000/5000 Loss: 3.17636 ACC: 86.578% p_acc: 96.356% AUC: 0.957\n",
      "train_batch 3500/5000 Loss: 3.24689 ACC: 86.390% p_acc: 96.278% AUC: 0.956\n",
      "train_batch 4000/5000 Loss: 3.27743 ACC: 86.221% p_acc: 96.210% AUC: 0.956\n",
      "train_batch 4500/5000 Loss: 3.29941 ACC: 85.944% p_acc: 96.149% AUC: 0.955\n",
      "train_batch 5000/5000 Loss: 3.31161 ACC: 85.930% p_acc: 96.096% AUC: 0.955\n",
      "train Epoch Loss: 3.31161 ACC: 85.930% p_ACC: 96.096% AUC:  0.955\n",
      ".............\n",
      "val_batch 500/833 Loss: 41.22368 ACC: 44.133% p_acc: 74.837% AUC: 0.701\n",
      "val Epoch Loss: 41.73451 ACC: 44.738% p_ACC: 74.775% AUC:  0.704\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 57/500------------\n",
      "train_batch 500/5000 Loss: 2.41226 ACC: 88.933% p_acc: 96.782% AUC: 0.966\n",
      "train_batch 1000/5000 Loss: 2.76989 ACC: 87.783% p_acc: 96.581% AUC: 0.961\n",
      "train_batch 1500/5000 Loss: 2.87240 ACC: 87.356% p_acc: 96.428% AUC: 0.960\n",
      "train_batch 2000/5000 Loss: 2.87597 ACC: 87.392% p_acc: 96.365% AUC: 0.961\n",
      "train_batch 2500/5000 Loss: 2.96266 ACC: 87.133% p_acc: 96.320% AUC: 0.960\n",
      "train_batch 3000/5000 Loss: 3.06692 ACC: 86.817% p_acc: 96.261% AUC: 0.959\n",
      "train_batch 3500/5000 Loss: 3.08441 ACC: 86.814% p_acc: 96.217% AUC: 0.959\n",
      "train_batch 4000/5000 Loss: 3.14500 ACC: 86.558% p_acc: 96.178% AUC: 0.958\n",
      "train_batch 4500/5000 Loss: 3.17439 ACC: 86.485% p_acc: 96.143% AUC: 0.957\n",
      "train_batch 5000/5000 Loss: 3.25073 ACC: 86.197% p_acc: 96.106% AUC: 0.956\n",
      "train Epoch Loss: 3.25073 ACC: 86.197% p_ACC: 96.106% AUC:  0.956\n",
      ".............\n",
      "val_batch 500/833 Loss: 37.10253 ACC: 45.667% p_acc: 75.454% AUC: 0.711\n",
      "val Epoch Loss: 37.98537 ACC: 45.898% p_ACC: 75.452% AUC:  0.712\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 58/500------------\n",
      "train_batch 500/5000 Loss: 2.67252 ACC: 88.133% p_acc: 96.355% AUC: 0.966\n",
      "train_batch 1000/5000 Loss: 2.86791 ACC: 87.733% p_acc: 96.377% AUC: 0.962\n",
      "train_batch 1500/5000 Loss: 2.92530 ACC: 87.378% p_acc: 96.319% AUC: 0.961\n",
      "train_batch 2000/5000 Loss: 2.95749 ACC: 87.342% p_acc: 96.267% AUC: 0.961\n",
      "train_batch 2500/5000 Loss: 2.96745 ACC: 87.287% p_acc: 96.225% AUC: 0.960\n",
      "train_batch 3000/5000 Loss: 3.09695 ACC: 86.883% p_acc: 96.189% AUC: 0.959\n",
      "train_batch 3500/5000 Loss: 3.13581 ACC: 86.605% p_acc: 96.148% AUC: 0.958\n",
      "train_batch 4000/5000 Loss: 3.17933 ACC: 86.558% p_acc: 96.110% AUC: 0.957\n",
      "train_batch 4500/5000 Loss: 3.22763 ACC: 86.407% p_acc: 96.075% AUC: 0.957\n",
      "train_batch 5000/5000 Loss: 3.26027 ACC: 86.353% p_acc: 96.044% AUC: 0.957\n",
      "train Epoch Loss: 3.26027 ACC: 86.353% p_ACC: 96.044% AUC:  0.957\n",
      ".............\n",
      "val_batch 500/833 Loss: 41.21559 ACC: 44.467% p_acc: 74.365% AUC: 0.702\n",
      "val Epoch Loss: 40.25409 ACC: 44.498% p_ACC: 74.449% AUC:  0.702\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 59/500------------\n",
      "train_batch 500/5000 Loss: 2.64934 ACC: 89.067% p_acc: 96.736% AUC: 0.968\n",
      "train_batch 1000/5000 Loss: 2.60193 ACC: 88.850% p_acc: 96.720% AUC: 0.966\n",
      "train_batch 1500/5000 Loss: 2.72211 ACC: 88.289% p_acc: 96.657% AUC: 0.964\n",
      "train_batch 2000/5000 Loss: 2.79892 ACC: 87.933% p_acc: 96.585% AUC: 0.963\n",
      "train_batch 2500/5000 Loss: 2.88516 ACC: 87.693% p_acc: 96.518% AUC: 0.962\n",
      "train_batch 3000/5000 Loss: 2.94293 ACC: 87.611% p_acc: 96.463% AUC: 0.961\n",
      "train_batch 3500/5000 Loss: 2.99333 ACC: 87.400% p_acc: 96.417% AUC: 0.960\n",
      "train_batch 4000/5000 Loss: 3.02500 ACC: 87.308% p_acc: 96.375% AUC: 0.960\n",
      "train_batch 5000/5000 Loss: 3.14408 ACC: 86.787% p_acc: 96.293% AUC: 0.958\n",
      "train Epoch Loss: 3.14408 ACC: 86.787% p_ACC: 96.293% AUC:  0.958\n",
      ".............\n",
      "val_batch 500/833 Loss: 38.65434 ACC: 42.500% p_acc: 73.931% AUC: 0.694\n",
      "val Epoch Loss: 38.49676 ACC: 43.517% p_ACC: 74.055% AUC:  0.699\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 60/500------------\n",
      "train_batch 500/5000 Loss: 2.81024 ACC: 89.033% p_acc: 96.791% AUC: 0.966\n",
      "train_batch 1000/5000 Loss: 2.67646 ACC: 88.633% p_acc: 96.695% AUC: 0.965\n",
      "train_batch 1500/5000 Loss: 2.74358 ACC: 88.444% p_acc: 96.640% AUC: 0.965\n",
      "train_batch 2000/5000 Loss: 2.83413 ACC: 87.933% p_acc: 96.593% AUC: 0.963\n",
      "train_batch 2500/5000 Loss: 2.91015 ACC: 87.447% p_acc: 96.527% AUC: 0.961\n",
      "train_batch 3000/5000 Loss: 2.97445 ACC: 87.144% p_acc: 96.462% AUC: 0.960\n",
      "train_batch 3500/5000 Loss: 3.03088 ACC: 87.062% p_acc: 96.405% AUC: 0.960\n",
      "train_batch 4000/5000 Loss: 3.07156 ACC: 86.950% p_acc: 96.358% AUC: 0.959\n",
      "train_batch 4500/5000 Loss: 3.14173 ACC: 86.626% p_acc: 96.314% AUC: 0.958\n",
      "train_batch 5000/5000 Loss: 3.16532 ACC: 86.597% p_acc: 96.272% AUC: 0.958\n",
      "train Epoch Loss: 3.16532 ACC: 86.597% p_ACC: 96.272% AUC:  0.958\n",
      ".............\n",
      "val_batch 500/833 Loss: 40.16889 ACC: 46.467% p_acc: 76.456% AUC: 0.724\n",
      "val Epoch Loss: 41.01690 ACC: 46.379% p_ACC: 76.258% AUC:  0.720\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 61/500------------\n",
      "train_batch 500/5000 Loss: 2.48275 ACC: 88.967% p_acc: 97.070% AUC: 0.969\n",
      "train_batch 1000/5000 Loss: 2.71701 ACC: 88.150% p_acc: 96.839% AUC: 0.965\n",
      "train_batch 1500/5000 Loss: 2.74267 ACC: 88.156% p_acc: 96.705% AUC: 0.964\n",
      "train_batch 2000/5000 Loss: 2.79632 ACC: 87.908% p_acc: 96.630% AUC: 0.963\n",
      "train_batch 2500/5000 Loss: 2.83963 ACC: 87.833% p_acc: 96.570% AUC: 0.963\n",
      "train_batch 3000/5000 Loss: 2.91882 ACC: 87.600% p_acc: 96.514% AUC: 0.962\n",
      "train_batch 3500/5000 Loss: 3.01522 ACC: 87.243% p_acc: 96.459% AUC: 0.960\n",
      "train_batch 4000/5000 Loss: 3.07462 ACC: 87.042% p_acc: 96.408% AUC: 0.959\n",
      "train_batch 4500/5000 Loss: 3.08732 ACC: 86.941% p_acc: 96.361% AUC: 0.959\n",
      "train_batch 5000/5000 Loss: 3.09512 ACC: 86.857% p_acc: 96.322% AUC: 0.958\n",
      "train Epoch Loss: 3.09512 ACC: 86.857% p_ACC: 96.322% AUC:  0.958\n",
      ".............\n",
      "val_batch 500/833 Loss: 42.30258 ACC: 44.200% p_acc: 74.265% AUC: 0.701\n",
      "val Epoch Loss: 40.85076 ACC: 45.778% p_ACC: 74.576% AUC:  0.710\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 62/500------------\n",
      "train_batch 500/5000 Loss: 2.66518 ACC: 88.933% p_acc: 97.177% AUC: 0.967\n",
      "train_batch 1000/5000 Loss: 2.65738 ACC: 88.333% p_acc: 96.874% AUC: 0.965\n",
      "train_batch 1500/5000 Loss: 2.65732 ACC: 88.411% p_acc: 96.771% AUC: 0.966\n",
      "train_batch 2000/5000 Loss: 2.75843 ACC: 88.133% p_acc: 96.712% AUC: 0.964\n",
      "train_batch 2500/5000 Loss: 2.89338 ACC: 87.700% p_acc: 96.649% AUC: 0.963\n",
      "train_batch 3000/5000 Loss: 2.99337 ACC: 87.378% p_acc: 96.580% AUC: 0.961\n",
      "train_batch 3500/5000 Loss: 2.99618 ACC: 87.281% p_acc: 96.521% AUC: 0.961\n",
      "train_batch 4000/5000 Loss: 3.01421 ACC: 87.183% p_acc: 96.470% AUC: 0.960\n",
      "train_batch 4500/5000 Loss: 3.07840 ACC: 86.985% p_acc: 96.425% AUC: 0.959\n",
      "train_batch 5000/5000 Loss: 3.13291 ACC: 86.823% p_acc: 96.381% AUC: 0.958\n",
      "train Epoch Loss: 3.13291 ACC: 86.823% p_ACC: 96.381% AUC:  0.958\n",
      ".............\n",
      "val_batch 500/833 Loss: 38.95841 ACC: 44.833% p_acc: 74.910% AUC: 0.706\n",
      "val Epoch Loss: 39.99519 ACC: 44.178% p_ACC: 74.834% AUC:  0.701\n",
      ".............\n",
      "-----------------Epoch cost time 215s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 63/500------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_batch 500/5000 Loss: 2.87020 ACC: 88.300% p_acc: 96.339% AUC: 0.964\n",
      "train_batch 1000/5000 Loss: 2.88991 ACC: 88.383% p_acc: 96.413% AUC: 0.963\n",
      "train_batch 1500/5000 Loss: 2.92842 ACC: 88.044% p_acc: 96.384% AUC: 0.962\n",
      "train_batch 2000/5000 Loss: 2.98716 ACC: 87.650% p_acc: 96.354% AUC: 0.961\n",
      "train_batch 2500/5000 Loss: 3.00011 ACC: 87.707% p_acc: 96.319% AUC: 0.961\n",
      "train_batch 3000/5000 Loss: 3.05181 ACC: 87.417% p_acc: 96.285% AUC: 0.960\n",
      "train_batch 3500/5000 Loss: 3.06555 ACC: 87.252% p_acc: 96.252% AUC: 0.959\n",
      "train_batch 4000/5000 Loss: 3.12390 ACC: 87.088% p_acc: 96.217% AUC: 0.959\n",
      "train_batch 4500/5000 Loss: 3.17883 ACC: 86.859% p_acc: 96.183% AUC: 0.958\n",
      "train_batch 5000/5000 Loss: 3.20426 ACC: 86.760% p_acc: 96.151% AUC: 0.957\n",
      "train Epoch Loss: 3.20426 ACC: 86.760% p_ACC: 96.151% AUC:  0.957\n",
      ".............\n",
      "val_batch 500/833 Loss: 40.09130 ACC: 43.633% p_acc: 73.421% AUC: 0.695\n",
      "val Epoch Loss: 39.99520 ACC: 43.457% p_ACC: 73.637% AUC:  0.692\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 64/500------------\n",
      "train_batch 500/5000 Loss: 2.65423 ACC: 88.933% p_acc: 96.499% AUC: 0.967\n",
      "train_batch 1000/5000 Loss: 2.75667 ACC: 88.450% p_acc: 96.540% AUC: 0.965\n",
      "train_batch 1500/5000 Loss: 2.80487 ACC: 88.333% p_acc: 96.539% AUC: 0.965\n",
      "train_batch 2000/5000 Loss: 2.82860 ACC: 88.233% p_acc: 96.525% AUC: 0.963\n",
      "train_batch 2500/5000 Loss: 2.91167 ACC: 87.873% p_acc: 96.488% AUC: 0.962\n",
      "train_batch 3000/5000 Loss: 2.97813 ACC: 87.528% p_acc: 96.440% AUC: 0.960\n",
      "train_batch 3500/5000 Loss: 3.00514 ACC: 87.510% p_acc: 96.399% AUC: 0.960\n",
      "train_batch 4000/5000 Loss: 3.02995 ACC: 87.333% p_acc: 96.365% AUC: 0.960\n",
      "train_batch 4500/5000 Loss: 3.05558 ACC: 87.241% p_acc: 96.333% AUC: 0.960\n",
      "train_batch 5000/5000 Loss: 3.11006 ACC: 87.013% p_acc: 96.303% AUC: 0.959\n",
      "train Epoch Loss: 3.11006 ACC: 87.013% p_ACC: 96.303% AUC:  0.959\n",
      ".............\n",
      "val_batch 500/833 Loss: 38.99222 ACC: 44.933% p_acc: 74.494% AUC: 0.706\n",
      "val Epoch Loss: 38.91713 ACC: 44.738% p_ACC: 74.611% AUC:  0.705\n",
      ".............\n",
      "-----------------Epoch cost time 260s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 65/500------------\n",
      "train_batch 500/5000 Loss: 2.73701 ACC: 88.867% p_acc: 96.758% AUC: 0.966\n",
      "train_batch 1000/5000 Loss: 2.74897 ACC: 88.300% p_acc: 96.649% AUC: 0.964\n",
      "train_batch 1500/5000 Loss: 2.73477 ACC: 88.300% p_acc: 96.580% AUC: 0.964\n",
      "train_batch 2000/5000 Loss: 2.78580 ACC: 88.083% p_acc: 96.539% AUC: 0.963\n",
      "train_batch 2500/5000 Loss: 2.89193 ACC: 87.787% p_acc: 96.497% AUC: 0.962\n",
      "train_batch 3000/5000 Loss: 2.89410 ACC: 87.733% p_acc: 96.457% AUC: 0.961\n",
      "train_batch 3500/5000 Loss: 2.95733 ACC: 87.629% p_acc: 96.425% AUC: 0.961\n",
      "train_batch 4000/5000 Loss: 3.01433 ACC: 87.471% p_acc: 96.391% AUC: 0.960\n",
      "train_batch 4500/5000 Loss: 3.06958 ACC: 87.370% p_acc: 96.360% AUC: 0.960\n",
      "train_batch 5000/5000 Loss: 3.11406 ACC: 87.187% p_acc: 96.329% AUC: 0.959\n",
      "train Epoch Loss: 3.11406 ACC: 87.187% p_ACC: 96.329% AUC:  0.959\n",
      ".............\n",
      "val_batch 500/833 Loss: 37.99448 ACC: 43.333% p_acc: 74.051% AUC: 0.702\n",
      "val Epoch Loss: 38.57435 ACC: 43.818% p_ACC: 74.189% AUC:  0.704\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 66/500------------\n",
      "train_batch 500/5000 Loss: 2.28302 ACC: 90.067% p_acc: 96.900% AUC: 0.971\n",
      "train_batch 1000/5000 Loss: 2.48957 ACC: 89.617% p_acc: 96.938% AUC: 0.969\n",
      "train_batch 1500/5000 Loss: 2.52248 ACC: 89.244% p_acc: 96.893% AUC: 0.968\n",
      "train_batch 2000/5000 Loss: 2.67504 ACC: 88.833% p_acc: 96.843% AUC: 0.966\n",
      "train_batch 2500/5000 Loss: 2.68824 ACC: 88.500% p_acc: 96.788% AUC: 0.965\n",
      "train_batch 3000/5000 Loss: 2.74160 ACC: 88.361% p_acc: 96.737% AUC: 0.964\n",
      "train_batch 3500/5000 Loss: 2.84469 ACC: 88.071% p_acc: 96.688% AUC: 0.963\n",
      "train_batch 4000/5000 Loss: 2.96458 ACC: 87.792% p_acc: 96.637% AUC: 0.961\n",
      "train_batch 4500/5000 Loss: 3.01793 ACC: 87.607% p_acc: 96.588% AUC: 0.961\n",
      "train_batch 5000/5000 Loss: 3.04595 ACC: 87.503% p_acc: 96.543% AUC: 0.960\n",
      "train Epoch Loss: 3.04595 ACC: 87.503% p_ACC: 96.543% AUC:  0.960\n",
      ".............\n",
      "val_batch 500/833 Loss: 37.23829 ACC: 46.133% p_acc: 75.503% AUC: 0.708\n",
      "val Epoch Loss: 37.70871 ACC: 44.718% p_ACC: 75.132% AUC:  0.701\n",
      ".............\n",
      "-----------------Epoch cost time 262s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 67/500------------\n",
      "train_batch 500/5000 Loss: 2.46724 ACC: 89.467% p_acc: 96.941% AUC: 0.970\n",
      "train_batch 1000/5000 Loss: 2.62419 ACC: 88.967% p_acc: 96.864% AUC: 0.967\n",
      "train_batch 1500/5000 Loss: 2.68253 ACC: 88.778% p_acc: 96.802% AUC: 0.967\n",
      "train_batch 2000/5000 Loss: 2.75473 ACC: 88.583% p_acc: 96.756% AUC: 0.965\n",
      "train_batch 2500/5000 Loss: 2.78859 ACC: 88.520% p_acc: 96.716% AUC: 0.965\n",
      "train_batch 3000/5000 Loss: 2.84063 ACC: 88.172% p_acc: 96.673% AUC: 0.964\n",
      "train_batch 3500/5000 Loss: 2.88865 ACC: 88.081% p_acc: 96.629% AUC: 0.963\n",
      "train_batch 4000/5000 Loss: 2.94327 ACC: 87.875% p_acc: 96.587% AUC: 0.962\n",
      "train_batch 4500/5000 Loss: 3.00969 ACC: 87.722% p_acc: 96.547% AUC: 0.961\n",
      "train_batch 5000/5000 Loss: 3.07183 ACC: 87.517% p_acc: 96.509% AUC: 0.960\n",
      "train Epoch Loss: 3.07183 ACC: 87.517% p_ACC: 96.509% AUC:  0.960\n",
      ".............\n",
      "val_batch 500/833 Loss: 37.46111 ACC: 45.733% p_acc: 74.080% AUC: 0.707\n",
      "val Epoch Loss: 38.36100 ACC: 45.738% p_ACC: 74.296% AUC:  0.704\n",
      ".............\n",
      "-----------------Epoch cost time 260s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 68/500------------\n",
      "train_batch 500/5000 Loss: 2.60477 ACC: 89.467% p_acc: 96.831% AUC: 0.967\n",
      "train_batch 1000/5000 Loss: 2.74826 ACC: 89.250% p_acc: 96.748% AUC: 0.966\n",
      "train_batch 1500/5000 Loss: 2.78426 ACC: 88.933% p_acc: 96.717% AUC: 0.965\n",
      "train_batch 2000/5000 Loss: 2.78197 ACC: 88.842% p_acc: 96.686% AUC: 0.965\n",
      "train_batch 2500/5000 Loss: 2.82770 ACC: 88.613% p_acc: 96.651% AUC: 0.964\n",
      "train_batch 3000/5000 Loss: 2.88162 ACC: 88.461% p_acc: 96.615% AUC: 0.963\n",
      "train_batch 3500/5000 Loss: 2.93658 ACC: 88.114% p_acc: 96.580% AUC: 0.962\n",
      "train_batch 4000/5000 Loss: 3.00445 ACC: 87.846% p_acc: 96.541% AUC: 0.961\n",
      "train_batch 4500/5000 Loss: 3.01465 ACC: 87.737% p_acc: 96.502% AUC: 0.961\n",
      "train_batch 5000/5000 Loss: 3.06532 ACC: 87.510% p_acc: 96.468% AUC: 0.960\n",
      "train Epoch Loss: 3.06532 ACC: 87.510% p_ACC: 96.468% AUC:  0.960\n",
      ".............\n",
      "val_batch 500/833 Loss: 38.13623 ACC: 43.767% p_acc: 74.378% AUC: 0.698\n",
      "val Epoch Loss: 37.76772 ACC: 44.298% p_ACC: 74.336% AUC:  0.703\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 69/500------------\n",
      "train_batch 500/5000 Loss: 2.53759 ACC: 89.800% p_acc: 97.061% AUC: 0.969\n",
      "train_batch 1000/5000 Loss: 2.53355 ACC: 89.233% p_acc: 96.926% AUC: 0.967\n",
      "train_batch 1500/5000 Loss: 2.62173 ACC: 88.522% p_acc: 96.847% AUC: 0.965\n",
      "train_batch 2000/5000 Loss: 2.67123 ACC: 88.425% p_acc: 96.764% AUC: 0.964\n",
      "train_batch 2500/5000 Loss: 2.73058 ACC: 88.133% p_acc: 96.704% AUC: 0.963\n",
      "train_batch 3000/5000 Loss: 2.75726 ACC: 88.011% p_acc: 96.654% AUC: 0.963\n",
      "train_batch 3500/5000 Loss: 2.86968 ACC: 87.762% p_acc: 96.607% AUC: 0.962\n",
      "train_batch 4000/5000 Loss: 2.89812 ACC: 87.533% p_acc: 96.563% AUC: 0.961\n",
      "train_batch 4500/5000 Loss: 2.94186 ACC: 87.393% p_acc: 96.521% AUC: 0.961\n",
      "train_batch 5000/5000 Loss: 2.99484 ACC: 87.237% p_acc: 96.479% AUC: 0.959\n",
      "train Epoch Loss: 2.99484 ACC: 87.237% p_ACC: 96.479% AUC:  0.959\n",
      ".............\n",
      "val_batch 500/833 Loss: 39.43377 ACC: 45.200% p_acc: 74.550% AUC: 0.706\n",
      "val Epoch Loss: 38.98742 ACC: 45.218% p_ACC: 74.640% AUC:  0.707\n",
      ".............\n",
      "-----------------Epoch cost time 262s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 70/500------------\n",
      "train_batch 500/5000 Loss: 2.52541 ACC: 90.500% p_acc: 97.336% AUC: 0.971\n",
      "train_batch 1000/5000 Loss: 2.41657 ACC: 89.917% p_acc: 97.183% AUC: 0.970\n",
      "train_batch 1500/5000 Loss: 2.52367 ACC: 89.456% p_acc: 97.084% AUC: 0.968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_batch 2000/5000 Loss: 2.59610 ACC: 89.200% p_acc: 97.008% AUC: 0.967\n",
      "train_batch 2500/5000 Loss: 2.66836 ACC: 88.873% p_acc: 96.940% AUC: 0.966\n",
      "train_batch 3000/5000 Loss: 2.75654 ACC: 88.611% p_acc: 96.878% AUC: 0.965\n",
      "train_batch 3500/5000 Loss: 2.80369 ACC: 88.371% p_acc: 96.822% AUC: 0.964\n",
      "train_batch 4000/5000 Loss: 2.83404 ACC: 88.229% p_acc: 96.772% AUC: 0.963\n",
      "train_batch 4500/5000 Loss: 2.90753 ACC: 87.956% p_acc: 96.725% AUC: 0.962\n",
      "train_batch 5000/5000 Loss: 2.97092 ACC: 87.713% p_acc: 96.678% AUC: 0.961\n",
      "train Epoch Loss: 2.97092 ACC: 87.713% p_ACC: 96.678% AUC:  0.961\n",
      ".............\n",
      "val_batch 500/833 Loss: 40.37115 ACC: 45.900% p_acc: 76.048% AUC: 0.711\n",
      "val Epoch Loss: 40.61529 ACC: 44.738% p_ACC: 75.712% AUC:  0.706\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 71/500------------\n",
      "train_batch 500/5000 Loss: 2.33236 ACC: 90.000% p_acc: 96.949% AUC: 0.967\n",
      "train_batch 1000/5000 Loss: 2.48547 ACC: 89.600% p_acc: 96.883% AUC: 0.967\n",
      "train_batch 1500/5000 Loss: 2.59519 ACC: 89.333% p_acc: 96.829% AUC: 0.966\n",
      "train_batch 2000/5000 Loss: 2.63276 ACC: 88.808% p_acc: 96.773% AUC: 0.965\n",
      "train_batch 2500/5000 Loss: 2.71187 ACC: 88.540% p_acc: 96.722% AUC: 0.964\n",
      "train_batch 3000/5000 Loss: 2.75147 ACC: 88.417% p_acc: 96.685% AUC: 0.963\n",
      "train_batch 3500/5000 Loss: 2.81651 ACC: 88.214% p_acc: 96.645% AUC: 0.962\n",
      "train_batch 4000/5000 Loss: 2.87284 ACC: 87.879% p_acc: 96.601% AUC: 0.961\n",
      "train_batch 4500/5000 Loss: 2.91336 ACC: 87.748% p_acc: 96.561% AUC: 0.960\n",
      "train_batch 5000/5000 Loss: 2.96194 ACC: 87.510% p_acc: 96.523% AUC: 0.960\n",
      "train Epoch Loss: 2.96194 ACC: 87.510% p_ACC: 96.523% AUC:  0.960\n",
      ".............\n",
      "val_batch 500/833 Loss: 43.79403 ACC: 43.967% p_acc: 73.913% AUC: 0.699\n",
      "val Epoch Loss: 43.05215 ACC: 44.378% p_ACC: 74.147% AUC:  0.701\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 72/500------------\n",
      "train_batch 500/5000 Loss: 2.35364 ACC: 89.567% p_acc: 96.820% AUC: 0.970\n",
      "train_batch 1000/5000 Loss: 2.36097 ACC: 89.933% p_acc: 96.914% AUC: 0.971\n",
      "train_batch 1500/5000 Loss: 2.46080 ACC: 89.467% p_acc: 96.928% AUC: 0.969\n",
      "train_batch 2000/5000 Loss: 2.57843 ACC: 89.150% p_acc: 96.896% AUC: 0.967\n",
      "train_batch 2500/5000 Loss: 2.60181 ACC: 88.940% p_acc: 96.852% AUC: 0.966\n",
      "train_batch 3000/5000 Loss: 2.72271 ACC: 88.506% p_acc: 96.805% AUC: 0.965\n",
      "train_batch 3500/5000 Loss: 2.78170 ACC: 88.457% p_acc: 96.758% AUC: 0.964\n",
      "train_batch 4000/5000 Loss: 2.85336 ACC: 88.325% p_acc: 96.719% AUC: 0.964\n",
      "train_batch 4500/5000 Loss: 2.92696 ACC: 87.985% p_acc: 96.679% AUC: 0.963\n",
      "train_batch 5000/5000 Loss: 2.98234 ACC: 87.847% p_acc: 96.639% AUC: 0.962\n",
      "train Epoch Loss: 2.98234 ACC: 87.847% p_ACC: 96.639% AUC:  0.962\n",
      ".............\n",
      "val_batch 500/833 Loss: 39.83783 ACC: 46.067% p_acc: 74.610% AUC: 0.704\n",
      "val Epoch Loss: 39.38654 ACC: 46.038% p_ACC: 74.726% AUC:  0.704\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 73/500------------\n",
      "train_batch 500/5000 Loss: 2.52783 ACC: 90.200% p_acc: 97.151% AUC: 0.970\n",
      "train_batch 1000/5000 Loss: 2.53151 ACC: 89.700% p_acc: 97.032% AUC: 0.967\n",
      "train_batch 1500/5000 Loss: 2.66345 ACC: 89.167% p_acc: 96.915% AUC: 0.965\n",
      "train_batch 2000/5000 Loss: 2.66729 ACC: 88.808% p_acc: 96.829% AUC: 0.964\n",
      "train_batch 2500/5000 Loss: 2.71846 ACC: 88.567% p_acc: 96.763% AUC: 0.963\n",
      "train_batch 3000/5000 Loss: 2.83701 ACC: 88.272% p_acc: 96.704% AUC: 0.962\n",
      "train_batch 3500/5000 Loss: 2.88245 ACC: 88.157% p_acc: 96.650% AUC: 0.962\n",
      "train_batch 4000/5000 Loss: 2.88509 ACC: 88.092% p_acc: 96.603% AUC: 0.961\n",
      "train_batch 4500/5000 Loss: 2.93260 ACC: 87.896% p_acc: 96.563% AUC: 0.961\n",
      "train_batch 5000/5000 Loss: 2.95880 ACC: 87.767% p_acc: 96.524% AUC: 0.960\n",
      "train Epoch Loss: 2.95880 ACC: 87.767% p_ACC: 96.524% AUC:  0.960\n",
      ".............\n",
      "val_batch 500/833 Loss: 41.54742 ACC: 43.633% p_acc: 73.908% AUC: 0.692\n",
      "val Epoch Loss: 42.03232 ACC: 44.138% p_ACC: 73.870% AUC:  0.695\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 74/500------------\n",
      "train_batch 500/5000 Loss: 2.19431 ACC: 90.567% p_acc: 97.169% AUC: 0.972\n",
      "train_batch 1000/5000 Loss: 2.43700 ACC: 89.667% p_acc: 97.068% AUC: 0.968\n",
      "train_batch 1500/5000 Loss: 2.57356 ACC: 89.056% p_acc: 96.951% AUC: 0.966\n",
      "train_batch 2000/5000 Loss: 2.67092 ACC: 88.875% p_acc: 96.868% AUC: 0.966\n",
      "train_batch 2500/5000 Loss: 2.70910 ACC: 88.813% p_acc: 96.807% AUC: 0.966\n",
      "train_batch 3000/5000 Loss: 2.74748 ACC: 88.678% p_acc: 96.758% AUC: 0.965\n",
      "train_batch 3500/5000 Loss: 2.82353 ACC: 88.290% p_acc: 96.714% AUC: 0.963\n",
      "train_batch 4000/5000 Loss: 2.85896 ACC: 88.121% p_acc: 96.668% AUC: 0.963\n",
      "train_batch 4500/5000 Loss: 2.90545 ACC: 87.930% p_acc: 96.627% AUC: 0.962\n",
      "train_batch 5000/5000 Loss: 2.95261 ACC: 87.770% p_acc: 96.586% AUC: 0.961\n",
      "train Epoch Loss: 2.95261 ACC: 87.770% p_ACC: 96.586% AUC:  0.961\n",
      ".............\n",
      "val_batch 500/833 Loss: 37.32956 ACC: 44.333% p_acc: 73.956% AUC: 0.708\n",
      "val Epoch Loss: 37.93861 ACC: 44.198% p_ACC: 74.323% AUC:  0.706\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 75/500------------\n",
      "train_batch 500/5000 Loss: 2.36444 ACC: 90.233% p_acc: 97.308% AUC: 0.972\n",
      "train_batch 1000/5000 Loss: 2.45035 ACC: 89.917% p_acc: 97.158% AUC: 0.970\n",
      "train_batch 1500/5000 Loss: 2.45850 ACC: 89.756% p_acc: 97.086% AUC: 0.969\n",
      "train_batch 2000/5000 Loss: 2.59333 ACC: 89.158% p_acc: 97.011% AUC: 0.966\n",
      "train_batch 2500/5000 Loss: 2.61440 ACC: 89.187% p_acc: 96.942% AUC: 0.966\n",
      "train_batch 3000/5000 Loss: 2.74662 ACC: 88.672% p_acc: 96.884% AUC: 0.964\n",
      "train_batch 3500/5000 Loss: 2.84445 ACC: 88.324% p_acc: 96.819% AUC: 0.963\n",
      "train_batch 4000/5000 Loss: 2.87354 ACC: 88.217% p_acc: 96.760% AUC: 0.963\n",
      "train_batch 4500/5000 Loss: 2.92400 ACC: 88.033% p_acc: 96.710% AUC: 0.962\n",
      "train_batch 5000/5000 Loss: 2.95470 ACC: 87.933% p_acc: 96.665% AUC: 0.962\n",
      "train Epoch Loss: 2.95470 ACC: 87.933% p_ACC: 96.665% AUC:  0.962\n",
      ".............\n",
      "val_batch 500/833 Loss: 39.46922 ACC: 43.400% p_acc: 74.275% AUC: 0.696\n",
      "val Epoch Loss: 38.86612 ACC: 43.758% p_ACC: 74.269% AUC:  0.699\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 76/500------------\n",
      "train_batch 500/5000 Loss: 2.22413 ACC: 90.367% p_acc: 96.971% AUC: 0.971\n",
      "train_batch 1000/5000 Loss: 2.41628 ACC: 89.750% p_acc: 97.024% AUC: 0.970\n",
      "train_batch 1500/5000 Loss: 2.49091 ACC: 89.467% p_acc: 96.967% AUC: 0.969\n",
      "train_batch 2000/5000 Loss: 2.54471 ACC: 89.175% p_acc: 96.929% AUC: 0.967\n",
      "train_batch 2500/5000 Loss: 2.68769 ACC: 88.620% p_acc: 96.868% AUC: 0.965\n",
      "train_batch 3000/5000 Loss: 2.72835 ACC: 88.411% p_acc: 96.799% AUC: 0.964\n",
      "train_batch 3500/5000 Loss: 2.75878 ACC: 88.267% p_acc: 96.743% AUC: 0.963\n",
      "train_batch 4000/5000 Loss: 2.80175 ACC: 88.138% p_acc: 96.694% AUC: 0.962\n",
      "train_batch 4500/5000 Loss: 2.82872 ACC: 87.981% p_acc: 96.650% AUC: 0.962\n",
      "train_batch 5000/5000 Loss: 2.88594 ACC: 87.933% p_acc: 96.612% AUC: 0.962\n",
      "train Epoch Loss: 2.88594 ACC: 87.933% p_ACC: 96.612% AUC:  0.962\n",
      ".............\n",
      "val_batch 500/833 Loss: 38.90090 ACC: 44.133% p_acc: 73.660% AUC: 0.698\n",
      "val Epoch Loss: 37.95285 ACC: 44.358% p_ACC: 73.974% AUC:  0.705\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 77/500------------\n",
      "train_batch 500/5000 Loss: 2.26646 ACC: 91.033% p_acc: 97.253% AUC: 0.974\n",
      "train_batch 1000/5000 Loss: 2.28713 ACC: 90.817% p_acc: 97.268% AUC: 0.972\n",
      "train_batch 1500/5000 Loss: 2.44802 ACC: 89.922% p_acc: 97.214% AUC: 0.969\n",
      "train_batch 2000/5000 Loss: 2.59296 ACC: 89.550% p_acc: 97.137% AUC: 0.968\n",
      "train_batch 2500/5000 Loss: 2.71807 ACC: 89.080% p_acc: 97.053% AUC: 0.965\n",
      "train_batch 3000/5000 Loss: 2.71072 ACC: 89.000% p_acc: 96.981% AUC: 0.965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_batch 3500/5000 Loss: 2.80613 ACC: 88.652% p_acc: 96.921% AUC: 0.964\n",
      "train_batch 4000/5000 Loss: 2.82989 ACC: 88.621% p_acc: 96.865% AUC: 0.963\n",
      "train_batch 4500/5000 Loss: 2.81911 ACC: 88.530% p_acc: 96.817% AUC: 0.963\n",
      "train_batch 5000/5000 Loss: 2.86018 ACC: 88.370% p_acc: 96.776% AUC: 0.962\n",
      "train Epoch Loss: 2.86018 ACC: 88.370% p_ACC: 96.776% AUC:  0.962\n",
      ".............\n",
      "val_batch 500/833 Loss: 41.06756 ACC: 45.433% p_acc: 75.230% AUC: 0.705\n",
      "val Epoch Loss: 40.80408 ACC: 45.438% p_ACC: 75.293% AUC:  0.711\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 78/500------------\n",
      "train_batch 500/5000 Loss: 2.16917 ACC: 90.667% p_acc: 97.142% AUC: 0.971\n",
      "train_batch 1000/5000 Loss: 2.43898 ACC: 89.567% p_acc: 97.126% AUC: 0.967\n",
      "train_batch 1500/5000 Loss: 2.51450 ACC: 89.189% p_acc: 97.000% AUC: 0.967\n",
      "train_batch 2000/5000 Loss: 2.59610 ACC: 88.958% p_acc: 96.921% AUC: 0.966\n",
      "train_batch 2500/5000 Loss: 2.59220 ACC: 88.993% p_acc: 96.869% AUC: 0.966\n",
      "train_batch 3000/5000 Loss: 2.63440 ACC: 88.878% p_acc: 96.830% AUC: 0.966\n",
      "train_batch 3500/5000 Loss: 2.68920 ACC: 88.667% p_acc: 96.795% AUC: 0.965\n",
      "train_batch 4000/5000 Loss: 2.73820 ACC: 88.500% p_acc: 96.758% AUC: 0.964\n",
      "train_batch 4500/5000 Loss: 2.82939 ACC: 88.141% p_acc: 96.718% AUC: 0.962\n",
      "train_batch 5000/5000 Loss: 2.84751 ACC: 88.007% p_acc: 96.680% AUC: 0.962\n",
      "train Epoch Loss: 2.84751 ACC: 88.007% p_ACC: 96.680% AUC:  0.962\n",
      ".............\n",
      "val_batch 500/833 Loss: 42.29679 ACC: 45.267% p_acc: 75.093% AUC: 0.715\n",
      "val Epoch Loss: 42.96129 ACC: 45.318% p_ACC: 75.222% AUC:  0.715\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 79/500------------\n",
      "train_batch 500/5000 Loss: 2.11756 ACC: 90.833% p_acc: 97.230% AUC: 0.972\n",
      "train_batch 1000/5000 Loss: 2.38221 ACC: 90.317% p_acc: 97.178% AUC: 0.969\n",
      "train_batch 1500/5000 Loss: 2.50787 ACC: 89.778% p_acc: 97.094% AUC: 0.968\n",
      "train_batch 2000/5000 Loss: 2.59268 ACC: 89.425% p_acc: 97.014% AUC: 0.966\n",
      "train_batch 2500/5000 Loss: 2.67552 ACC: 89.127% p_acc: 96.948% AUC: 0.965\n",
      "train_batch 3000/5000 Loss: 2.74350 ACC: 88.906% p_acc: 96.884% AUC: 0.964\n",
      "train_batch 3500/5000 Loss: 2.77520 ACC: 88.714% p_acc: 96.826% AUC: 0.963\n",
      "train_batch 4000/5000 Loss: 2.80491 ACC: 88.521% p_acc: 96.778% AUC: 0.962\n",
      "train_batch 4500/5000 Loss: 2.85046 ACC: 88.404% p_acc: 96.736% AUC: 0.962\n",
      "train_batch 5000/5000 Loss: 2.89533 ACC: 88.267% p_acc: 96.697% AUC: 0.961\n",
      "train Epoch Loss: 2.89533 ACC: 88.267% p_ACC: 96.697% AUC:  0.961\n",
      ".............\n",
      "val_batch 500/833 Loss: 36.57217 ACC: 44.000% p_acc: 74.520% AUC: 0.698\n",
      "val Epoch Loss: 36.44144 ACC: 43.998% p_ACC: 74.452% AUC:  0.698\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 80/500------------\n",
      "train_batch 500/5000 Loss: 2.36688 ACC: 91.000% p_acc: 97.355% AUC: 0.974\n",
      "train_batch 1000/5000 Loss: 2.36151 ACC: 90.450% p_acc: 97.257% AUC: 0.971\n",
      "train_batch 1500/5000 Loss: 2.42977 ACC: 90.111% p_acc: 97.187% AUC: 0.969\n",
      "train_batch 2000/5000 Loss: 2.52189 ACC: 89.517% p_acc: 97.119% AUC: 0.968\n",
      "train_batch 2500/5000 Loss: 2.55544 ACC: 89.480% p_acc: 97.055% AUC: 0.968\n",
      "train_batch 3000/5000 Loss: 2.63588 ACC: 89.361% p_acc: 97.005% AUC: 0.967\n",
      "train_batch 3500/5000 Loss: 2.69275 ACC: 89.152% p_acc: 96.959% AUC: 0.966\n",
      "train_batch 4000/5000 Loss: 2.77798 ACC: 88.813% p_acc: 96.911% AUC: 0.964\n",
      "train_batch 4500/5000 Loss: 2.83891 ACC: 88.622% p_acc: 96.864% AUC: 0.964\n",
      "train_batch 5000/5000 Loss: 2.89119 ACC: 88.387% p_acc: 96.818% AUC: 0.963\n",
      "train Epoch Loss: 2.89119 ACC: 88.387% p_ACC: 96.818% AUC:  0.963\n",
      ".............\n",
      "val_batch 500/833 Loss: 40.07055 ACC: 45.967% p_acc: 75.470% AUC: 0.715\n",
      "val Epoch Loss: 40.63486 ACC: 45.858% p_ACC: 75.370% AUC:  0.715\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 81/500------------\n",
      "train_batch 500/5000 Loss: 2.30556 ACC: 90.767% p_acc: 97.139% AUC: 0.971\n",
      "train_batch 1000/5000 Loss: 2.32635 ACC: 90.467% p_acc: 97.130% AUC: 0.970\n",
      "train_batch 1500/5000 Loss: 2.47326 ACC: 89.800% p_acc: 97.093% AUC: 0.968\n",
      "train_batch 2000/5000 Loss: 2.41736 ACC: 89.875% p_acc: 97.048% AUC: 0.969\n",
      "train_batch 2500/5000 Loss: 2.57521 ACC: 89.500% p_acc: 97.007% AUC: 0.967\n",
      "train_batch 3000/5000 Loss: 2.64690 ACC: 89.378% p_acc: 96.967% AUC: 0.967\n",
      "train_batch 3500/5000 Loss: 2.70394 ACC: 89.038% p_acc: 96.921% AUC: 0.965\n",
      "train_batch 4000/5000 Loss: 2.73788 ACC: 88.838% p_acc: 96.878% AUC: 0.964\n",
      "train_batch 4500/5000 Loss: 2.78164 ACC: 88.622% p_acc: 96.838% AUC: 0.964\n",
      "train_batch 5000/5000 Loss: 2.84450 ACC: 88.433% p_acc: 96.801% AUC: 0.963\n",
      "train Epoch Loss: 2.84450 ACC: 88.433% p_ACC: 96.801% AUC:  0.963\n",
      ".............\n",
      "val_batch 500/833 Loss: 36.75349 ACC: 45.700% p_acc: 75.121% AUC: 0.714\n",
      "val Epoch Loss: 37.61659 ACC: 45.578% p_ACC: 75.127% AUC:  0.709\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 82/500------------\n",
      "train_batch 1000/5000 Loss: 2.54189 ACC: 90.367% p_acc: 96.916% AUC: 0.968\n",
      "train_batch 1500/5000 Loss: 2.55766 ACC: 90.267% p_acc: 96.928% AUC: 0.969\n",
      "train_batch 2000/5000 Loss: 2.66500 ACC: 89.833% p_acc: 96.914% AUC: 0.967\n",
      "train_batch 2500/5000 Loss: 2.72402 ACC: 89.533% p_acc: 96.879% AUC: 0.966\n",
      "train_batch 3000/5000 Loss: 2.75656 ACC: 89.350% p_acc: 96.842% AUC: 0.965\n",
      "train_batch 3500/5000 Loss: 2.81423 ACC: 89.067% p_acc: 96.803% AUC: 0.964\n",
      "train_batch 4000/5000 Loss: 2.82636 ACC: 89.058% p_acc: 96.770% AUC: 0.964\n",
      "train_batch 4500/5000 Loss: 2.87297 ACC: 88.933% p_acc: 96.742% AUC: 0.964\n",
      "train_batch 5000/5000 Loss: 2.90228 ACC: 88.827% p_acc: 96.714% AUC: 0.963\n",
      "train Epoch Loss: 2.90228 ACC: 88.827% p_ACC: 96.714% AUC:  0.963\n",
      ".............\n",
      "val_batch 500/833 Loss: 37.98998 ACC: 43.733% p_acc: 74.673% AUC: 0.699\n",
      "val Epoch Loss: 38.14459 ACC: 43.818% p_ACC: 74.626% AUC:  0.699\n",
      ".............\n",
      "-----------------Epoch cost time 261s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 83/500------------\n",
      "train_batch 500/5000 Loss: 2.16756 ACC: 90.767% p_acc: 97.319% AUC: 0.970\n",
      "train_batch 1000/5000 Loss: 2.31000 ACC: 90.500% p_acc: 97.212% AUC: 0.970\n",
      "train_batch 1500/5000 Loss: 2.46488 ACC: 90.033% p_acc: 97.149% AUC: 0.968\n",
      "train_batch 2000/5000 Loss: 2.56898 ACC: 89.800% p_acc: 97.077% AUC: 0.967\n",
      "train_batch 2500/5000 Loss: 2.60905 ACC: 89.613% p_acc: 97.015% AUC: 0.967\n",
      "train_batch 3000/5000 Loss: 2.67495 ACC: 89.350% p_acc: 96.964% AUC: 0.966\n",
      "train_batch 3500/5000 Loss: 2.74048 ACC: 89.152% p_acc: 96.918% AUC: 0.965\n",
      "train_batch 4000/5000 Loss: 2.79846 ACC: 88.833% p_acc: 96.875% AUC: 0.964\n",
      "train_batch 4500/5000 Loss: 2.81537 ACC: 88.752% p_acc: 96.831% AUC: 0.964\n",
      "train_batch 5000/5000 Loss: 2.86764 ACC: 88.607% p_acc: 96.792% AUC: 0.963\n",
      "train Epoch Loss: 2.86764 ACC: 88.607% p_ACC: 96.792% AUC:  0.963\n",
      ".............\n",
      "val_batch 500/833 Loss: 39.35550 ACC: 44.333% p_acc: 74.130% AUC: 0.700\n",
      "val Epoch Loss: 39.10538 ACC: 43.938% p_ACC: 74.121% AUC:  0.697\n",
      ".............\n",
      "-----------------Epoch cost time 197s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 84/500------------\n",
      "train_batch 500/5000 Loss: 2.47882 ACC: 90.033% p_acc: 96.954% AUC: 0.970\n",
      "train_batch 1000/5000 Loss: 2.45661 ACC: 90.100% p_acc: 96.988% AUC: 0.970\n",
      "train_batch 1500/5000 Loss: 2.48834 ACC: 89.933% p_acc: 96.952% AUC: 0.968\n",
      "train_batch 2000/5000 Loss: 2.56706 ACC: 89.542% p_acc: 96.922% AUC: 0.967\n",
      "train_batch 2500/5000 Loss: 2.58571 ACC: 89.413% p_acc: 96.883% AUC: 0.966\n",
      "train_batch 3000/5000 Loss: 2.68078 ACC: 89.211% p_acc: 96.844% AUC: 0.965\n",
      "train_batch 3500/5000 Loss: 2.72431 ACC: 89.110% p_acc: 96.809% AUC: 0.965\n",
      "train_batch 4000/5000 Loss: 2.78572 ACC: 88.942% p_acc: 96.780% AUC: 0.964\n",
      "train_batch 4500/5000 Loss: 2.81148 ACC: 88.793% p_acc: 96.749% AUC: 0.964\n",
      "train_batch 5000/5000 Loss: 2.85856 ACC: 88.687% p_acc: 96.720% AUC: 0.963\n",
      "train Epoch Loss: 2.85856 ACC: 88.687% p_ACC: 96.720% AUC:  0.963\n",
      ".............\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_batch 500/833 Loss: 38.16749 ACC: 42.867% p_acc: 74.676% AUC: 0.699\n",
      "val Epoch Loss: 39.52135 ACC: 42.597% p_ACC: 74.476% AUC:  0.699\n",
      ".............\n",
      "-----------------Epoch cost time 157s--------------------\n",
      "\n",
      "==================================================================\n",
      "-------------Epoch: 85/500------------\n",
      "train_batch 500/5000 Loss: 2.32550 ACC: 89.833% p_acc: 96.975% AUC: 0.969\n",
      "train_batch 1000/5000 Loss: 2.17140 ACC: 90.800% p_acc: 97.056% AUC: 0.971\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
